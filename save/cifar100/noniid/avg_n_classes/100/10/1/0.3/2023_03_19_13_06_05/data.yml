avg_train_accuracy: 0.333
avg_train_loss: 0.005
avg_type: avg_n_classes
dataset: cifar100
epochs: 100
frac: 0.3
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.001
max_pool: 'True'
model: cnn
momentum: 0.9
norm: batch_norm
num_channels: 3
num_classes: 100
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 10
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.0391
- 0.0893
- 0.1088
- 0.132
- 0.1455
- 0.1595
- 0.1691
- 0.1699
- 0.1816
- 0.1912
- 0.1997
- 0.2094
- 0.2093
- 0.2128
- 0.2138
- 0.2223
- 0.2247
- 0.2313
- 0.2313
- 0.2344
- 0.2366
- 0.2381
- 0.2414
- 0.2513
- 0.2475
- 0.2458
- 0.2546
- 0.2606
- 0.2551
- 0.2513
- 0.2587
- 0.2621
- 0.2614
- 0.2745
- 0.2705
- 0.2719
- 0.2737
- 0.2772
- 0.2796
- 0.2797
- 0.2779
- 0.2797
- 0.29
- 0.29
- 0.283
- 0.2874
- 0.282
- 0.2898
- 0.2923
- 0.2904
- 0.2934
- 0.2941
- 0.2889
- 0.2937
- 0.2892
- 0.2908
- 0.3018
- 0.2968
- 0.2994
- 0.3035
- 0.3009
- 0.3028
- 0.3012
- 0.2997
- 0.2984
- 0.3016
- 0.3055
- 0.3006
- 0.3069
- 0.3037
- 0.303
- 0.307
- 0.3056
- 0.2994
- 0.3074
- 0.3112
- 0.3051
- 0.3113
- 0.3115
- 0.3153
- 0.3137
- 0.3115
- 0.3092
- 0.3081
- 0.3134
- 0.3121
- 0.314
- 0.3132
- 0.313
- 0.312
- 0.3176
- 0.3125
- 0.3173
- 0.3142
- 0.3202
- 0.3185
- 0.3075
- 0.3117
- 0.3168
- 0.3189
test_loss_list:
- 1.7989317846298218
- 1.6827217483520507
- 1.6282979011535645
- 1.6042563247680663
- 1.5804420161247252
- 1.5636948490142821
- 1.5554315519332886
- 1.549540672302246
- 1.5029297184944153
- 1.5019577717781067
- 1.492267780303955
- 1.4831653237342834
- 1.4879448461532592
- 1.4848129272460937
- 1.4434665775299071
- 1.4102396750450135
- 1.4150459170341492
- 1.385818166732788
- 1.3970215153694152
- 1.4071088767051696
- 1.4076029205322265
- 1.411386127471924
- 1.4066384434700012
- 1.401755907535553
- 1.3648384881019593
- 1.3507182502746582
- 1.3595518445968628
- 1.3657947063446045
- 1.3752395224571228
- 1.3487390875816345
- 1.355907940864563
- 1.3218879461288453
- 1.3080453372001648
- 1.319544985294342
- 1.3017356491088867
- 1.3207584476470948
- 1.2904100036621093
- 1.3019471526145936
- 1.3121151804924012
- 1.3278129076957703
- 1.291460018157959
- 1.2765073037147523
- 1.2850090599060058
- 1.3032798886299133
- 1.275530436038971
- 1.2909680581092835
- 1.2748886227607727
- 1.2799259948730468
- 1.278360414505005
- 1.3009180641174316
- 1.294947693347931
- 1.2640170764923095
- 1.2500660872459413
- 1.274642720222473
- 1.293972384929657
- 1.2603207397460938
- 1.2619302105903625
- 1.2479442501068114
- 1.2597446036338806
- 1.2723485350608825
- 1.2812516570091248
- 1.2792482328414918
- 1.253148729801178
- 1.2380890226364136
- 1.266706576347351
- 1.2752647972106934
- 1.271667444705963
- 1.2927461552619934
- 1.2917750310897826
- 1.3015285730361938
- 1.2595383095741273
- 1.2682922339439393
- 1.2828630137443542
- 1.2492889857292175
- 1.2652891397476196
- 1.2710204076766969
- 1.2820606422424317
- 1.2830930137634278
- 1.2870626521110535
- 1.2861709094047546
- 1.2948109221458435
- 1.3153141593933106
- 1.306394762992859
- 1.2715487027168273
- 1.292000048160553
- 1.2913676738739013
- 1.2923215103149415
- 1.297141845226288
- 1.2619835686683656
- 1.233827509880066
- 1.2618152546882628
- 1.2738963508605956
- 1.2729017424583435
- 1.2947365307807923
- 1.287884500026703
- 1.29852352142334
- 1.2681985449790956
- 1.2823724460601806
- 1.2930075407028199
- 1.244156928062439
train_accuracy:
- 0.041
- 0.091
- 0.117
- 0.169
- 0.146
- 0.179
- 0.14
- 0.153
- 0.171
- 0.173
- 0.218
- 0.198
- 0.259
- 0.215
- 0.194
- 0.258
- 0.236
- 0.221
- 0.235
- 0.217
- 0.223
- 0.265
- 0.233
- 0.24
- 0.243
- 0.0
- 0.281
- 0.254
- 0.253
- 0.212
- 0.294
- 0.244
- 0.236
- 0.265
- 0.314
- 0.243
- 0.284
- 0.273
- 0.241
- 0.296
- 0.291
- 0.265
- 0.3
- 0.289
- 0.277
- 0.326
- 0.277
- 0.279
- 0.276
- 0.302
- 0.277
- 0.0
- 0.298
- 0.297
- 0.35
- 0.0
- 0.329
- 0.286
- 0.316
- 0.305
- 0.282
- 0.324
- 0.321
- 0.0
- 0.316
- 0.307
- 0.328
- 0.291
- 0.317
- 0.306
- 0.267
- 0.284
- 0.354
- 0.268
- 0.322
- 0.356
- 0.354
- 0.335
- 0.32
- 0.312
- 0.33
- 0.34
- 0.304
- 0.318
- 0.306
- 0.322
- 0.34
- 0.342
- 0.0
- 0.327
- 0.333
- 0.318
- 0.301
- 0.342
- 0.37
- 0.318
- 0.364
- 0.339
- 0.305
- 0.333
train_loss:
- 3.441
- 3.847
- 2.935
- 3.408
- 3.278
- 3.074
- 2.985
- 2.895
- 2.55
- 2.744
- 2.731
- 2.696
- 2.601
- 2.412
- 2.187
- 2.08
- 2.316
- 2.078
- 2.164
- 2.095
- 2.243
- 2.104
- 2.006
- 2.124
- 1.825
- 1.648
- 1.773
- 1.794
- 1.615
- 1.399
- 1.68
- 1.528
- 1.533
- 1.673
- 1.364
- 1.397
- 1.431
- 1.576
- 1.374
- 1.412
- 1.302
- 1.248
- 1.314
- 1.361
- 1.117
- 1.135
- 1.097
- 1.317
- 1.094
- 1.212
- 1.014
- 1.132
- 0.888
- 0.979
- 0.878
- 0.971
- 1.138
- 0.903
- 1.001
- 0.886
- 0.943
- 0.796
- 0.88
- 0.818
- 0.819
- 0.884
- 0.745
- 0.697
- 0.702
- 0.727
- 0.737
- 0.739
- 0.741
- 0.748
- 0.62
- 0.637
- 0.677
- 0.566
- 0.515
- 0.606
- 0.494
- 0.5
- 0.536
- 0.616
- 0.456
- 0.422
- 0.443
- 0.478
- 0.555
- 0.555
- 0.6
- 0.42
- 0.398
- 0.378
- 0.417
- 0.372
- 0.507
- 0.345
- 0.33
- 0.526
unequal: 0
verbose: 1
