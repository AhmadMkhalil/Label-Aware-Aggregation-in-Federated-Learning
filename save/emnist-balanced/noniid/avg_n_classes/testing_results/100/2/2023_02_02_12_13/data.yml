avg_train_accuracy: 0.0
avg_train_loss: 0.006
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.04452127659574468
- 0.1901595744680851
- 0.23542553191489363
- 0.30042553191489363
- 0.3075
- 0.3298404255319149
- 0.34191489361702126
- 0.3536170212765957
- 0.3646276595744681
- 0.3646276595744681
- 0.36792553191489363
- 0.3747340425531915
- 0.376063829787234
- 0.38430851063829785
- 0.3878191489361702
- 0.3878191489361702
- 0.3900531914893617
- 0.3911702127659574
- 0.3911702127659574
- 0.3911702127659574
- 0.39372340425531915
- 0.4022340425531915
- 0.4022340425531915
- 0.40595744680851065
- 0.40595744680851065
- 0.409468085106383
- 0.4102659574468085
- 0.41085106382978726
- 0.41085106382978726
- 0.41117021276595744
- 0.41308510638297874
- 0.4152127659574468
- 0.4160106382978723
- 0.4160106382978723
- 0.4197872340425532
- 0.4197872340425532
- 0.420531914893617
- 0.420531914893617
- 0.42143617021276597
- 0.42143617021276597
- 0.42143617021276597
- 0.4223404255319149
- 0.4223404255319149
- 0.4226063829787234
- 0.4226063829787234
- 0.42281914893617023
- 0.42425531914893616
- 0.42425531914893616
- 0.4270212765957447
- 0.4270212765957447
- 0.4270212765957447
- 0.4270212765957447
- 0.4270212765957447
- 0.4279787234042553
- 0.4279787234042553
- 0.4279787234042553
- 0.4279787234042553
- 0.4297872340425532
- 0.4297872340425532
- 0.4297872340425532
- 0.43
- 0.43
- 0.43
- 0.4304787234042553
- 0.4304787234042553
- 0.4318085106382979
- 0.4318085106382979
- 0.4318085106382979
- 0.4318085106382979
- 0.4320212765957447
- 0.4325
- 0.4325
- 0.4325
- 0.4325
- 0.4325
- 0.43351063829787234
- 0.43351063829787234
- 0.43351063829787234
- 0.43351063829787234
- 0.43351063829787234
- 0.43351063829787234
- 0.433936170212766
- 0.433936170212766
- 0.433936170212766
- 0.433936170212766
- 0.4347872340425532
- 0.4347872340425532
- 0.4347872340425532
- 0.4347872340425532
- 0.4347872340425532
- 0.4347872340425532
- 0.4348936170212766
- 0.4348936170212766
- 0.4348936170212766
- 0.4348936170212766
- 0.4348936170212766
- 0.4348936170212766
- 0.4348936170212766
- 0.4351063829787234
- 0.4351063829787234
test_loss_list:
- 584.958277463913
- 665.5344114303589
- 776.9476084709167
- 841.2455387115479
- 802.8010067939758
- 817.4964590072632
- 709.3237023353577
- 791.4461240768433
- 883.9977555274963
- 883.9977555274963
- 781.7629504203796
- 812.8786897659302
- 713.5631642341614
- 800.9538769721985
- 794.2850804328918
- 794.2850804328918
- 820.8059248924255
- 759.8576922416687
- 759.8576922416687
- 759.8576922416687
- 740.5157716274261
- 896.9003262519836
- 896.9003262519836
- 884.1502771377563
- 884.1502771377563
- 912.9508256912231
- 929.4055399894714
- 850.9609265327454
- 850.9609265327454
- 752.0119779109955
- 832.9757905006409
- 831.5849986076355
- 816.5250120162964
- 816.5250120162964
- 841.2388653755188
- 841.2388653755188
- 924.984580039978
- 924.984580039978
- 939.0381789207458
- 939.0381789207458
- 939.0381789207458
- 769.6598467826843
- 769.6598467826843
- 938.0497889518738
- 938.0497889518738
- 829.1198329925537
- 850.9666886329651
- 850.9666886329651
- 954.9750871658325
- 954.9750871658325
- 954.9750871658325
- 954.9750871658325
- 954.9750871658325
- 936.961793422699
- 936.961793422699
- 936.961793422699
- 936.961793422699
- 967.3480710983276
- 967.3480710983276
- 967.3480710983276
- 959.9735674858093
- 959.9735674858093
- 959.9735674858093
- 977.5973782539368
- 977.5973782539368
- 949.5885529518127
- 949.5885529518127
- 949.5885529518127
- 949.5885529518127
- 959.131528377533
- 965.6639695167542
- 965.6639695167542
- 965.6639695167542
- 965.6639695167542
- 965.6639695167542
- 963.7775373458862
- 963.7775373458862
- 963.7775373458862
- 963.7775373458862
- 963.7775373458862
- 963.7775373458862
- 960.4351801872253
- 960.4351801872253
- 960.4351801872253
- 960.4351801872253
- 938.2814259529114
- 938.2814259529114
- 938.2814259529114
- 938.2814259529114
- 938.2814259529114
- 938.2814259529114
- 944.1624794006348
- 944.1624794006348
- 944.1624794006348
- 944.1624794006348
- 944.1624794006348
- 944.1624794006348
- 944.1624794006348
- 815.1858711242676
- 815.1858711242676
train_accuracy:
- 0.085
- 0.354
- 0.506
- 0.623
- 0.0
- 0.675
- 0.0
- 0.771
- 0.754
- 0.0
- 0.756
- 0.71
- 0.0
- 0.796
- 0.0
- 0.817
- 0.81
- 0.835
- 0.0
- 0.0
- 0.76
- 0.854
- 0.833
- 0.783
- 0.0
- 0.835
- 0.84
- 0.852
- 0.0
- 0.865
- 0.794
- 0.85
- 0.879
- 0.846
- 0.0
- 0.781
- 0.877
- 0.852
- 0.84
- 0.0
- 0.0
- 0.879
- 0.875
- 0.873
- 0.871
- 0.881
- 0.86
- 0.871
- 0.896
- 0.0
- 0.0
- 0.0
- 0.0
- 0.906
- 0.858
- 0.865
- 0.9
- 0.871
- 0.89
- 0.0
- 0.894
- 0.883
- 0.89
- 0.894
- 0.0
- 0.85
- 0.906
- 0.0
- 0.0
- 0.879
- 0.913
- 0.862
- 0.906
- 0.915
- 0.904
- 0.9
- 0.862
- 0.0
- 0.0
- 0.89
- 0.883
- 0.896
- 0.904
- 0.892
- 0.0
- 0.908
- 0.902
- 0.898
- 0.906
- 0.898
- 0.9
- 0.898
- 0.0
- 0.85
- 0.858
- 0.0
- 0.877
- 0.0
- 0.898
- 0.0
train_loss:
- 3.014
- 2.584
- 1.586
- 2.302
- 1.673
- 1.526
- 1.18
- 1.338
- 1.571
- 1.073
- 1.241
- 1.21
- 0.942
- 1.128
- 1.144
- 1.073
- 1.093
- 0.874
- 0.82
- 0.692
- 0.869
- 1.12
- 0.828
- 1.099
- 0.94
- 1.043
- 1.018
- 0.875
- 0.775
- 0.727
- 0.825
- 0.882
- 0.909
- 0.861
- 0.817
- 0.789
- 0.926
- 0.787
- 0.886
- 0.712
- 0.766
- 0.647
- 0.761
- 0.858
- 0.752
- 0.81
- 0.776
- 0.739
- 0.833
- 0.753
- 0.736
- 0.629
- 0.711
- 0.814
- 0.794
- 0.742
- 0.711
- 0.777
- 0.713
- 0.73
- 0.769
- 0.754
- 0.765
- 0.753
- 0.628
- 0.775
- 0.685
- 0.698
- 0.638
- 0.756
- 0.746
- 0.674
- 0.66
- 0.693
- 0.71
- 0.729
- 0.704
- 0.635
- 0.638
- 0.65
- 0.668
- 0.72
- 0.639
- 0.727
- 0.723
- 0.72
- 0.674
- 0.666
- 0.677
- 0.638
- 0.64
- 0.701
- 0.672
- 0.66
- 0.568
- 0.602
- 0.67
- 0.604
- 0.645
- 0.646
unequal: 0
verbose: 1
