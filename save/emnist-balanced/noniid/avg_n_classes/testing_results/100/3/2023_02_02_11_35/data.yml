avg_train_accuracy: 0.0
avg_train_loss: 0.005
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 3
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.08037234042553192
- 0.11154255319148935
- 0.14308510638297872
- 0.15436170212765957
- 0.16132978723404257
- 0.17287234042553193
- 0.18106382978723404
- 0.18521276595744682
- 0.1875
- 0.19085106382978723
- 0.19085106382978723
- 0.19085106382978723
- 0.19085106382978723
- 0.19085106382978723
- 0.19085106382978723
- 0.19085106382978723
- 0.19585106382978723
- 0.1976063829787234
- 0.1976063829787234
- 0.1976063829787234
- 0.1998936170212766
- 0.1998936170212766
- 0.1998936170212766
- 0.1998936170212766
- 0.20122340425531915
- 0.20191489361702128
- 0.20191489361702128
- 0.20840425531914894
- 0.20840425531914894
- 0.20840425531914894
- 0.20840425531914894
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.21329787234042552
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.23159574468085106
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.2352659574468085
- 0.255
- 0.255
- 0.255
- 0.255
- 0.255
- 0.255
- 0.255
test_loss_list:
- 812.2970476150513
- 1182.5359120368958
- 954.1479525566101
- 930.8580694198608
- 844.2005372047424
- 853.4349870681763
- 1464.8952293395996
- 1494.166256904602
- 1494.3861055374146
- 1521.6660070419312
- 1521.6660070419312
- 1521.6660070419312
- 1521.6660070419312
- 1521.6660070419312
- 1521.6660070419312
- 1521.6660070419312
- 1377.2982769012451
- 1471.535855293274
- 1471.535855293274
- 1471.535855293274
- 1518.8353872299194
- 1518.8353872299194
- 1518.8353872299194
- 1518.8353872299194
- 1499.5137977600098
- 1517.9495725631714
- 1517.9495725631714
- 765.5278835296631
- 765.5278835296631
- 765.5278835296631
- 765.5278835296631
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 713.8484110832214
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 696.529764175415
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 585.7513673305511
- 611.8562955856323
- 611.8562955856323
- 611.8562955856323
- 611.8562955856323
- 611.8562955856323
- 611.8562955856323
- 611.8562955856323
train_accuracy:
- 0.0
- 0.508
- 0.61
- 0.0
- 0.731
- 0.0
- 0.808
- 0.812
- 0.821
- 0.854
- 0.733
- 0.0
- 0.762
- 0.79
- 0.012
- 0.833
- 0.846
- 0.848
- 0.731
- 0.829
- 0.86
- 0.006
- 0.862
- 0.823
- 0.871
- 0.881
- 0.875
- 0.85
- 0.86
- 0.004
- 0.871
- 0.894
- 0.873
- 0.844
- 0.894
- 0.033
- 0.0
- 0.865
- 0.91
- 0.885
- 0.86
- 0.896
- 0.896
- 0.0
- 0.888
- 0.892
- 0.885
- 0.883
- 0.908
- 0.885
- 0.873
- 0.873
- 0.019
- 0.888
- 0.883
- 0.9
- 0.0
- 0.894
- 0.89
- 0.871
- 0.921
- 0.913
- 0.896
- 0.917
- 0.0
- 0.917
- 0.873
- 0.875
- 0.0
- 0.048
- 0.898
- 0.904
- 0.0
- 0.908
- 0.0
- 0.923
- 0.115
- 0.879
- 0.904
- 0.915
- 0.002
- 0.908
- 0.898
- 0.896
- 0.908
- 0.908
- 0.902
- 0.906
- 0.0
- 0.102
- 0.921
- 0.877
- 0.919
- 0.142
- 0.898
- 0.09
- 0.888
- 0.91
- 0.002
- 0.0
train_loss:
- 2.166
- 1.708
- 1.387
- 1.244
- 1.173
- 1.066
- 1.137
- 1.061
- 1.03
- 0.987
- 1.012
- 0.872
- 0.89
- 0.837
- 1.007
- 0.785
- 0.867
- 0.805
- 0.881
- 0.788
- 0.778
- 0.776
- 0.671
- 0.793
- 0.739
- 0.694
- 0.757
- 0.595
- 0.769
- 0.631
- 0.625
- 0.519
- 0.584
- 0.76
- 0.638
- 0.822
- 0.661
- 0.616
- 0.619
- 0.588
- 0.6
- 0.533
- 0.585
- 0.708
- 0.584
- 0.568
- 0.532
- 0.551
- 0.536
- 0.522
- 0.653
- 0.614
- 0.532
- 0.662
- 0.588
- 0.533
- 0.566
- 0.515
- 0.547
- 0.659
- 0.509
- 0.581
- 0.529
- 0.491
- 0.666
- 0.521
- 0.646
- 0.578
- 0.467
- 0.64
- 0.503
- 0.54
- 0.494
- 0.529
- 0.553
- 0.491
- 0.556
- 0.502
- 0.557
- 0.474
- 0.678
- 0.501
- 0.503
- 0.483
- 0.464
- 0.514
- 0.483
- 0.555
- 0.568
- 0.508
- 0.466
- 0.572
- 0.508
- 0.513
- 0.458
- 0.498
- 0.499
- 0.527
- 0.518
- 0.523
unequal: 0
verbose: 1
