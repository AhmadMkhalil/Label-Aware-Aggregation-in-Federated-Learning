avg_train_accuracy: 0.927
avg_train_loss: 0.001
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 0
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.027659574468085105
- 0.03425531914893617
- 0.03425531914893617
- 0.03425531914893617
- 0.03425531914893617
- 0.03425531914893617
- 0.03425531914893617
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.036170212765957444
- 0.03914893617021276
- 0.03914893617021276
- 0.03914893617021276
- 0.03914893617021276
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.042925531914893615
- 0.04329787234042553
- 0.04452127659574468
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.0449468085106383
- 0.05191489361702128
- 0.05191489361702128
- 0.05191489361702128
- 0.05191489361702128
- 0.05191489361702128
- 0.05191489361702128
- 0.05191489361702128
- 0.052393617021276595
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
- 0.0673936170212766
test_loss_list:
- 898.6953611373901
- 977.2782182693481
- 977.2782182693481
- 977.2782182693481
- 977.2782182693481
- 977.2782182693481
- 977.2782182693481
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1061.17662191391
- 1224.5684895515442
- 1224.5684895515442
- 1224.5684895515442
- 1224.5684895515442
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 851.6895308494568
- 911.7893719673157
- 830.5279178619385
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 744.4602856636047
- 779.6997952461243
- 779.6997952461243
- 779.6997952461243
- 779.6997952461243
- 779.6997952461243
- 779.6997952461243
- 779.6997952461243
- 749.9713687896729
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
- 723.2193465232849
train_accuracy:
- 0.956
- 0.61
- 0.99
- 1.0
- 0.0
- 1.0
- 0.0
- 0.0
- 0.185
- 0.0
- 0.0
- 0.0
- 0.996
- 0.0
- 0.004
- 0.158
- 0.548
- 0.0
- 0.0
- 1.0
- 0.75
- 0.0
- 1.0
- 0.006
- 0.873
- 0.0
- 0.925
- 0.0
- 0.719
- 0.04
- 0.0
- 0.921
- 0.0
- 0.0
- 0.0
- 0.66
- 0.0
- 1.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.002
- 0.969
- 0.006
- 1.0
- 0.0
- 0.906
- 0.671
- 0.852
- 0.0
- 0.046
- 1.0
- 1.0
- 0.0
- 1.0
- 0.0
- 0.008
- 0.0
- 0.983
- 0.0
- 0.977
- 0.083
- 0.002
- 0.994
- 0.994
- 0.865
- 0.39
- 0.975
- 0.865
- 0.685
- 0.0
- 0.271
- 0.0
- 0.99
- 0.267
- 0.0
- 0.131
- 0.827
- 0.027
- 0.844
- 0.0
- 0.006
- 0.0
- 0.452
- 0.669
- 0.0
- 0.898
- 0.017
- 0.473
- 0.842
- 0.0
- 0.0
- 0.188
- 0.967
- 0.348
- 0.112
- 0.458
- 0.025
- 0.927
train_loss:
- 0.362
- 0.131
- 0.144
- 0.131
- 0.145
- 0.162
- 0.147
- 0.134
- 0.186
- 0.103
- 0.129
- 0.127
- 0.113
- 0.155
- 0.165
- 0.122
- 0.148
- 0.12
- 0.148
- 0.118
- 0.082
- 0.17
- 0.126
- 0.103
- 0.141
- 0.108
- 0.09
- 0.115
- 0.083
- 0.116
- 0.136
- 0.12
- 0.109
- 0.111
- 0.122
- 0.142
- 0.13
- 0.111
- 0.12
- 0.159
- 0.096
- 0.083
- 0.108
- 0.138
- 0.11
- 0.119
- 0.136
- 0.119
- 0.127
- 0.126
- 0.106
- 0.088
- 0.127
- 0.153
- 0.091
- 0.117
- 0.114
- 0.109
- 0.105
- 0.105
- 0.112
- 0.074
- 0.104
- 0.099
- 0.115
- 0.099
- 0.102
- 0.1
- 0.068
- 0.074
- 0.065
- 0.087
- 0.085
- 0.086
- 0.064
- 0.099
- 0.088
- 0.129
- 0.1
- 0.076
- 0.094
- 0.117
- 0.108
- 0.112
- 0.107
- 0.108
- 0.083
- 0.121
- 0.067
- 0.09
- 0.044
- 0.128
- 0.068
- 0.122
- 0.115
- 0.07
- 0.098
- 0.087
- 0.072
- 0.11
unequal: 0
verbose: 1
