avg_train_accuracy: 0.863
avg_train_loss: 0.005
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.5
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 3
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.04143617021276596
- 0.19079787234042553
- 0.2743617021276596
- 0.3159574468085106
- 0.11558510638297873
- 0.34393617021276596
- 0.35712765957446807
- 0.36696808510638296
- 0.3824468085106383
- 0.18207446808510638
- 0.3955851063829787
- 0.3996808510638298
- 0.40106382978723404
- 0.403031914893617
- 0.41069148936170213
- 0.4118085106382979
- 0.4171808510638298
- 0.4288297872340426
- 0.4178723404255319
- 0.4297340425531915
- 0.4327127659574468
- 0.43420212765957444
- 0.4328723404255319
- 0.44
- 0.4403723404255319
- 0.4404787234042553
- 0.4470212765957447
- 0.4477659574468085
- 0.4473936170212766
- 0.45313829787234045
- 0.44643617021276594
- 0.4530851063829787
- 0.45404255319148934
- 0.45845744680851064
- 0.4538829787234043
- 0.3058510638297872
- 0.46835106382978725
- 0.34691489361702127
- 0.4747872340425532
- 0.475
- 0.47111702127659577
- 0.4647340425531915
- 0.4631382978723404
- 0.4697340425531915
- 0.47138297872340423
- 0.465
- 0.4710106382978723
- 0.47212765957446806
- 0.4732446808510638
- 0.47388297872340424
- 0.4819148936170213
- 0.47085106382978725
- 0.47377659574468084
- 0.4795744680851064
- 0.46664893617021275
- 0.47606382978723405
- 0.478031914893617
- 0.4808510638297872
- 0.4777127659574468
- 0.3798936170212766
- 0.4771808510638298
- 0.4221808510638298
- 0.4846276595744681
- 0.485531914893617
- 0.4786170212765957
- 0.4811170212765957
- 0.4740957446808511
- 0.4785106382978723
- 0.43207446808510636
- 0.4826063829787234
- 0.4844148936170213
- 0.4903723404255319
- 0.48648936170212764
- 0.4848404255319149
- 0.48329787234042554
- 0.4928723404255319
- 0.4862234042553191
- 0.48143617021276597
- 0.48356382978723406
- 0.4779255319148936
- 0.4856382978723404
- 0.4872872340425532
- 0.48377659574468085
- 0.48601063829787233
- 0.49425531914893617
- 0.48164893617021276
- 0.4807446808510638
- 0.4931382978723404
- 0.4908510638297872
- 0.4872872340425532
- 0.49372340425531913
- 0.49117021276595746
- 0.49579787234042555
- 0.4927127659574468
- 0.48164893617021276
- 0.48851063829787233
- 0.4825
- 0.48414893617021276
- 0.4879787234042553
- 0.48617021276595745
test_loss_list:
- 3.8718225288391115
- 3.9086478169759116
- 3.8369633769989013
- 3.487431224187215
- 3.848638509114583
- 3.2299568875630698
- 3.1902468172709146
- 3.2231922276814777
- 3.283578249613444
- 3.348259023030599
- 3.1044445864359536
- 3.14392404238383
- 2.9413527488708495
- 3.0697323513031005
- 3.2639010111490885
- 2.9580347601572674
- 3.0804598045349123
- 3.4009883244832357
- 2.9926416301727294
- 3.151526225407918
- 3.0304896227518716
- 3.0877075799306235
- 2.936224610010783
- 3.116094875335693
- 2.9568622398376463
- 2.8537740008036296
- 3.3100564511617026
- 3.04259960492452
- 2.826682627995809
- 3.065415376027425
- 2.7294467894236245
- 2.963560005823771
- 2.7149174626668295
- 2.935734666188558
- 2.7085117626190187
- 2.7023189640045167
- 2.491578337351481
- 2.4948933760325116
- 2.40278920173645
- 2.426938591003418
- 2.4892681948343913
- 2.723909107844035
- 2.6311226050059
- 2.4230304018656414
- 2.5067873096466062
- 2.672745434443156
- 2.4103924655914306
- 2.714296245574951
- 2.522909777959188
- 2.571909974416097
- 2.451142349243164
- 2.415468200047811
- 2.995810381571452
- 2.7114937750498456
- 2.3496621958414714
- 2.7071771812438965
- 2.368276392618815
- 2.551832284927368
- 2.6598105494181317
- 2.156260363260905
- 2.361917667388916
- 2.0790081055959067
- 2.309159992535909
- 2.473259131113688
- 2.537847506205241
- 2.310430014928182
- 2.527153844833374
- 2.5766486994425457
- 1.8930275964736938
- 2.3045930004119874
- 2.4311979452768964
- 2.455986026128133
- 2.266766781806946
- 2.5462681516011556
- 2.537160587310791
- 2.573385171890259
- 2.3498113377889
- 2.6089363384246824
- 2.291759599049886
- 2.263241147994995
- 2.2375589561462403
- 2.221098405520121
- 2.8378451379140217
- 2.482029806772868
- 2.294115964571635
- 2.842416648864746
- 2.419972276687622
- 2.435819040934245
- 2.556793756484985
- 2.241774428685506
- 2.268698296546936
- 2.517337586085002
- 2.280240006446838
- 2.433621072769165
- 2.4554099527994793
- 2.15296284198761
- 2.7293662389119464
- 2.499370450973511
- 2.151916793187459
- 2.4434940338134767
train_accuracy:
- 0.0
- 0.0
- 0.506
- 0.0
- 0.046
- 0.0
- 0.0
- 0.685
- 0.0
- 0.325
- 0.0
- 0.735
- 0.723
- 0.0
- 0.0
- 0.0
- 0.0
- 0.792
- 0.763
- 0.0
- 0.802
- 0.802
- 0.796
- 0.0
- 0.0
- 0.0
- 0.819
- 0.798
- 0.0
- 0.819
- 0.0
- 0.0
- 0.027
- 0.838
- 0.819
- 0.008
- 0.0
- 0.654
- 0.0
- 0.0
- 0.825
- 0.848
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.019
- 0.856
- 0.0
- 0.071
- 0.0
- 0.844
- 0.0
- 0.0
- 0.84
- 0.0
- 0.002
- 0.04
- 0.681
- 0.842
- 0.69
- 0.0
- 0.004
- 0.0
- 0.0
- 0.852
- 0.863
- 0.265
- 0.0
- 0.86
- 0.031
- 0.004
- 0.0
- 0.865
- 0.048
- 0.002
- 0.0
- 0.0
- 0.0
- 0.863
- 0.069
- 0.006
- 0.067
- 0.0
- 0.0
- 0.867
- 0.002
- 0.877
- 0.85
- 0.002
- 0.873
- 0.0
- 0.879
- 0.873
- 0.033
- 0.867
- 0.86
- 0.0
- 0.863
train_loss:
- 2.173
- 1.852
- 1.09
- 0.937
- 0.567
- 0.832
- 0.857
- 1.051
- 0.972
- 0.455
- 0.939
- 0.901
- 0.73
- 0.739
- 0.874
- 0.675
- 0.579
- 1.001
- 0.598
- 0.759
- 0.771
- 0.749
- 0.565
- 0.749
- 0.547
- 0.544
- 0.857
- 0.671
- 0.53
- 0.635
- 0.53
- 0.699
- 0.51
- 0.648
- 0.491
- 0.361
- 0.477
- 0.292
- 0.454
- 0.44
- 0.438
- 0.607
- 0.596
- 0.454
- 0.427
- 0.599
- 0.447
- 0.565
- 0.423
- 0.432
- 0.423
- 0.435
- 0.659
- 0.529
- 0.483
- 0.543
- 0.463
- 0.399
- 0.537
- 0.316
- 0.581
- 0.27
- 0.542
- 0.527
- 0.515
- 0.413
- 0.503
- 0.503
- 0.262
- 0.538
- 0.505
- 0.488
- 0.382
- 0.515
- 0.474
- 0.471
- 0.404
- 0.497
- 0.395
- 0.394
- 0.381
- 0.353
- 0.563
- 0.499
- 0.386
- 0.571
- 0.526
- 0.493
- 0.473
- 0.387
- 0.386
- 0.476
- 0.369
- 0.465
- 0.466
- 0.382
- 0.569
- 0.465
- 0.404
- 0.472
unequal: 0
verbose: 1
