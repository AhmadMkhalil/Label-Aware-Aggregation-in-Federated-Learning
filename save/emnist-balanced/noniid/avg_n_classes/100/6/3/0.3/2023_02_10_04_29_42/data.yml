avg_train_accuracy: 0.856
avg_train_loss: 0.007
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.3
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 6
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.04824468085106383
- 0.09212765957446808
- 0.16170212765957448
- 0.2533510638297872
- 0.2968085106382979
- 0.33627659574468083
- 0.37813829787234043
- 0.34632978723404256
- 0.40367021276595744
- 0.3880851063829787
- 0.4047340425531915
- 0.40925531914893615
- 0.43
- 0.41367021276595745
- 0.4331382978723404
- 0.4352127659574468
- 0.451063829787234
- 0.4422340425531915
- 0.44851063829787235
- 0.46037234042553193
- 0.4423936170212766
- 0.459468085106383
- 0.46962765957446806
- 0.47069148936170213
- 0.46420212765957447
- 0.47191489361702127
- 0.47904255319148936
- 0.474468085106383
- 0.47382978723404257
- 0.476968085106383
- 0.4822872340425532
- 0.48021276595744683
- 0.48909574468085104
- 0.4873936170212766
- 0.49319148936170215
- 0.4953723404255319
- 0.4913297872340426
- 0.4965425531914894
- 0.4930851063829787
- 0.4996808510638298
- 0.5003723404255319
- 0.49686170212765957
- 0.5005851063829787
- 0.49792553191489364
- 0.4984574468085106
- 0.5037234042553191
- 0.5070212765957447
- 0.505904255319149
- 0.5036170212765958
- 0.5060106382978723
- 0.5086702127659575
- 0.5046276595744681
- 0.5061170212765957
- 0.4981914893617021
- 0.512127659574468
- 0.511436170212766
- 0.5097340425531914
- 0.5148404255319149
- 0.5134574468085107
- 0.5136702127659575
- 0.5176595744680851
- 0.5097872340425532
- 0.5169680851063829
- 0.515531914893617
- 0.5286702127659575
- 0.5164893617021277
- 0.5334574468085106
- 0.5337234042553192
- 0.530904255319149
- 0.5219680851063829
- 0.5177127659574469
- 0.5193617021276595
- 0.5245744680851064
- 0.5196276595744681
- 0.5243085106382979
- 0.5267021276595745
- 0.5175
- 0.5186170212765957
- 0.5265425531914893
- 0.525904255319149
- 0.5206382978723404
- 0.5493617021276596
- 0.5330851063829787
- 0.5459574468085107
- 0.5295212765957447
- 0.5341489361702128
- 0.5218085106382979
- 0.5384574468085106
- 0.5388297872340425
- 0.5363297872340426
- 0.5303723404255319
- 0.543563829787234
- 0.5638297872340425
- 0.5269680851063829
- 0.5232446808510638
- 0.5234042553191489
- 0.5238297872340425
- 0.5254787234042553
- 0.5284042553191489
- 0.53
test_loss_list:
- 3.8350787576039633
- 3.815278882980347
- 3.623245808283488
- 3.399712375005086
- 3.2001704374949136
- 3.0916211382548013
- 4.089393672943115
- 2.9735922463734945
- 4.266049769719442
- 3.3072488180796307
- 3.46873220761617
- 3.4582471497853597
- 4.323035513559978
- 3.3202712758382162
- 3.340648988087972
- 3.431625000635783
- 4.480800517400106
- 3.354334888458252
- 3.2900234127044676
- 4.430688505172729
- 2.7355770111083983
- 3.3600392309824625
- 4.398399111429851
- 4.535783042907715
- 3.2618607107798256
- 3.4783681297302245
- 4.365768801371257
- 3.336037864685059
- 3.2704677391052246
- 2.6956425507863364
- 3.16472980817159
- 2.568194770812988
- 4.122755502065023
- 3.036264009475708
- 4.204138768513998
- 4.390783917109172
- 3.2258806387583414
- 3.290194511413574
- 3.2860915756225584
- 4.337946548461914
- 3.1886230150858563
- 3.1922583103179933
- 4.20426462173462
- 3.1376704502105714
- 3.273824742635091
- 4.161740395228068
- 4.310372021993001
- 4.430161848068237
- 3.3199477100372317
- 4.384094737370809
- 4.43722549756368
- 3.289645258585612
- 3.40431427637736
- 2.5562364292144775
- 2.556081631978353
- 2.454393917719523
- 3.9721663665771483
- 2.9507122961680095
- 4.078733081817627
- 4.120049266815186
- 2.4972399361928304
- 2.9997528044382733
- 3.0229479281107583
- 2.9487797005971275
- 2.315775890350342
- 3.908745056788127
- 2.2503771034876507
- 2.419275302886963
- 2.900923070907593
- 2.7116876665751137
- 3.8619386609395345
- 2.894857110977173
- 2.9998439947764077
- 4.093501351674398
- 2.945057830810547
- 3.0853552850087484
- 2.8889276027679442
- 4.015017277399699
- 3.02197021484375
- 3.0813030274709066
- 2.8862864112854005
- 2.401803878148397
- 2.9439233938852944
- 2.208551621437073
- 2.9130633799235026
- 2.830629377365112
- 3.7709883085886635
- 2.289571762084961
- 2.218661484718323
- 2.7950027147928873
- 2.7581751537323
- 2.7974821376800536
- 2.3488974237442015
- 3.861467761993408
- 3.9366503143310547
- 2.7719905694325764
- 3.9314264742533367
- 4.142667824427287
- 2.976462917327881
- 3.145561761856079
train_accuracy:
- 0.0
- 0.0
- 0.0
- 0.006
- 0.002
- 0.0
- 0.619
- 0.577
- 0.66
- 0.675
- 0.698
- 0.675
- 0.727
- 0.0
- 0.0
- 0.0
- 0.748
- 0.0
- 0.715
- 0.796
- 0.0
- 0.771
- 0.775
- 0.779
- 0.783
- 0.758
- 0.821
- 0.794
- 0.0
- 0.0
- 0.808
- 0.775
- 0.798
- 0.0
- 0.821
- 0.817
- 0.817
- 0.806
- 0.812
- 0.835
- 0.825
- 0.823
- 0.804
- 0.8
- 0.804
- 0.848
- 0.825
- 0.838
- 0.827
- 0.86
- 0.844
- 0.846
- 0.0
- 0.0
- 0.0
- 0.0
- 0.848
- 0.0
- 0.85
- 0.842
- 0.0
- 0.0
- 0.842
- 0.84
- 0.0
- 0.854
- 0.835
- 0.85
- 0.881
- 0.0
- 0.86
- 0.0
- 0.0
- 0.846
- 0.854
- 0.842
- 0.881
- 0.842
- 0.85
- 0.0
- 0.858
- 0.0
- 0.863
- 0.002
- 0.887
- 0.869
- 0.879
- 0.004
- 0.004
- 0.867
- 0.0
- 0.856
- 0.0
- 0.856
- 0.887
- 0.827
- 0.898
- 0.896
- 0.904
- 0.856
train_loss:
- 3.133
- 2.295
- 1.978
- 1.69
- 1.472
- 1.344
- 2.199
- 1.468
- 1.972
- 1.642
- 1.538
- 1.446
- 1.718
- 1.482
- 1.43
- 1.361
- 1.58
- 1.299
- 1.316
- 1.475
- 1.138
- 1.228
- 1.405
- 1.391
- 1.233
- 1.15
- 1.317
- 1.133
- 1.142
- 0.967
- 1.07
- 0.909
- 1.245
- 1.074
- 1.236
- 1.188
- 1.052
- 0.978
- 1.017
- 1.172
- 0.986
- 1.011
- 1.134
- 1.004
- 0.939
- 1.116
- 1.099
- 1.072
- 0.939
- 1.065
- 1.076
- 0.917
- 0.875
- 0.888
- 0.734
- 0.769
- 1.033
- 0.906
- 1.028
- 1.003
- 0.753
- 0.903
- 0.865
- 0.891
- 0.714
- 0.987
- 0.757
- 0.666
- 0.84
- 0.855
- 0.962
- 0.825
- 0.804
- 0.961
- 0.806
- 0.787
- 0.824
- 0.961
- 0.801
- 0.771
- 0.86
- 0.644
- 0.791
- 0.672
- 0.775
- 0.785
- 0.917
- 0.665
- 0.654
- 0.782
- 0.753
- 0.781
- 0.617
- 0.92
- 0.898
- 0.798
- 0.887
- 0.864
- 0.758
- 0.74
unequal: 0
verbose: 1
