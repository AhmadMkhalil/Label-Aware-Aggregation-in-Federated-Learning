avg_train_accuracy: 0.692
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.7
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 6
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.040106382978723404
- 0.10452127659574469
- 0.12484042553191489
- 0.15696808510638297
- 0.12771276595744682
- 0.18515957446808512
- 0.2277127659574468
- 0.23090425531914893
- 0.2623404255319149
- 0.24
- 0.22648936170212766
- 0.25968085106382977
- 0.2543617021276596
- 0.26585106382978724
- 0.3077127659574468
- 0.3191489361702128
- 0.3032446808510638
- 0.23351063829787233
- 0.2823936170212766
- 0.3495744680851064
- 0.28840425531914893
- 0.3596808510638298
- 0.3756914893617021
- 0.3696808510638298
- 0.3777659574468085
- 0.3698936170212766
- 0.35781914893617023
- 0.31670212765957445
- 0.40430851063829787
- 0.3626595744680851
- 0.3629787234042553
- 0.38143617021276593
- 0.391968085106383
- 0.39882978723404255
- 0.40712765957446806
- 0.35436170212765955
- 0.37382978723404253
- 0.3828723404255319
- 0.3929255319148936
- 0.4229255319148936
- 0.40117021276595743
- 0.4434574468085106
- 0.4529787234042553
- 0.42579787234042554
- 0.43861702127659574
- 0.4346276595744681
- 0.4211170212765957
- 0.4318085106382979
- 0.4233510638297872
- 0.4330851063829787
- 0.40388297872340423
- 0.43388297872340426
- 0.44648936170212766
- 0.4049468085106383
- 0.3878723404255319
- 0.46585106382978725
- 0.40122340425531916
- 0.47664893617021276
- 0.4046276595744681
- 0.4656914893617021
- 0.4628191489361702
- 0.4920744680851064
- 0.416968085106383
- 0.49888297872340426
- 0.5074468085106383
- 0.4627659574468085
- 0.4670744680851064
- 0.46367021276595743
- 0.421968085106383
- 0.48186170212765955
- 0.4320212765957447
- 0.4750531914893617
- 0.4052127659574468
- 0.44845744680851063
- 0.4685106382978723
- 0.5068617021276596
- 0.4208510638297872
- 0.48164893617021276
- 0.48590425531914894
- 0.5067021276595745
- 0.4850531914893617
- 0.4452659574468085
- 0.46345744680851064
- 0.5107446808510638
- 0.5137765957446808
- 0.42340425531914894
- 0.475
- 0.5104255319148936
- 0.48819148936170215
- 0.48840425531914894
- 0.5293085106382979
- 0.4748936170212766
- 0.4574468085106383
- 0.4854255319148936
- 0.47973404255319146
- 0.4854787234042553
- 0.5120744680851064
- 0.5393085106382979
- 0.5204255319148936
- 0.5008510638297873
test_loss_list:
- 4.149097347259522
- 4.013011064529419
- 3.58866579691569
- 3.3981229972839357
- 3.399395761489868
- 3.3533444945017497
- 3.226837402979533
- 3.2814337889353435
- 3.215929902394613
- 3.1536471748352053
- 3.285767037073771
- 3.095920213063558
- 3.339883499145508
- 3.113509823481242
- 3.1417810916900635
- 2.9778587341308596
- 3.3448177782694497
- 3.2499263858795167
- 2.996937068303426
- 2.9641957887013755
- 3.1752765560150147
- 2.7835727373758954
- 2.7612506357828774
- 2.939113292694092
- 3.0141776688893636
- 2.92863463083903
- 2.8335042254130047
- 3.1125704511006673
- 2.7094705359141034
- 2.8778672981262208
- 2.9788615258534747
- 2.9508319250742594
- 2.8067983373006187
- 2.833021256128947
- 2.8190685176849364
- 3.2304587109883625
- 2.764950984319051
- 2.9896966711680095
- 2.8355412387847903
- 2.7344041633605958
- 2.7704693508148193
- 2.6777314949035644
- 2.5982455730438234
- 2.7351267306009928
- 2.640900688171387
- 2.658614355723063
- 2.857576576868693
- 2.7500515429178876
- 2.791673380533854
- 2.6313003476460777
- 2.857165797551473
- 2.6879829216003417
- 2.747469409306844
- 2.991999594370524
- 3.155712842941284
- 2.636943941116333
- 3.0288485431671144
- 2.7201969305674236
- 3.041325750350952
- 2.7046027342478434
- 2.696561272939046
- 2.7014834531148275
- 3.167661050160726
- 2.614849650065104
- 2.5734215561548868
- 2.911269086201986
- 2.6933697764078777
- 2.725964902242025
- 3.0346264775594074
- 2.5720434252421063
- 2.8449782371520995
- 2.661779152552287
- 2.990535519917806
- 2.9642398993174237
- 2.7518840916951497
- 2.589420062700907
- 3.095804281234741
- 2.609052937825521
- 2.6839260705312094
- 2.65116104443868
- 2.7721390533447265
- 2.9295512676239013
- 2.8452054691314697
- 2.627395857175191
- 2.648954300880432
- 3.0187046750386557
- 2.724517879486084
- 2.662004680633545
- 2.8095475832621255
- 2.7703363132476806
- 2.6085487318038942
- 2.942396640777588
- 2.9346341546376546
- 2.7571577803293863
- 2.804369265238444
- 2.888279619216919
- 2.808301960627238
- 2.5547806294759114
- 2.7348661915461223
- 2.720849075317383
train_accuracy:
- 0.0
- 0.152
- 0.029
- 0.06
- 0.01
- 0.206
- 0.071
- 0.079
- 0.023
- 0.062
- 0.027
- 0.042
- 0.146
- 0.025
- 0.077
- 0.106
- 0.494
- 0.05
- 0.021
- 0.537
- 0.075
- 0.065
- 0.06
- 0.517
- 0.14
- 0.085
- 0.496
- 0.131
- 0.129
- 0.113
- 0.065
- 0.483
- 0.096
- 0.446
- 0.123
- 0.46
- 0.135
- 0.104
- 0.525
- 0.129
- 0.129
- 0.1
- 0.065
- 0.142
- 0.519
- 0.108
- 0.079
- 0.083
- 0.088
- 0.071
- 0.125
- 0.115
- 0.106
- 0.727
- 0.596
- 0.065
- 0.121
- 0.071
- 0.565
- 0.094
- 0.117
- 0.137
- 0.113
- 0.092
- 0.104
- 0.117
- 0.113
- 0.633
- 0.123
- 0.123
- 0.696
- 0.135
- 0.06
- 0.121
- 0.081
- 0.123
- 0.692
- 0.14
- 0.596
- 0.671
- 0.119
- 0.669
- 0.142
- 0.048
- 0.119
- 0.113
- 0.1
- 0.715
- 0.64
- 0.66
- 0.715
- 0.71
- 0.721
- 0.61
- 0.129
- 0.113
- 0.125
- 0.152
- 0.117
- 0.692
train_loss:
- 1.854
- 1.137
- 1.038
- 0.937
- 0.882
- 0.828
- 0.779
- 0.738
- 0.704
- 0.7
- 0.688
- 0.684
- 0.638
- 0.658
- 0.599
- 0.626
- 0.583
- 0.639
- 0.615
- 0.564
- 0.576
- 0.578
- 0.551
- 0.545
- 0.505
- 0.509
- 0.543
- 0.512
- 0.515
- 0.518
- 0.492
- 0.485
- 0.513
- 0.483
- 0.498
- 0.479
- 0.511
- 0.446
- 0.497
- 0.49
- 0.472
- 0.461
- 0.453
- 0.473
- 0.46
- 0.446
- 0.449
- 0.462
- 0.451
- 0.443
- 0.463
- 0.444
- 0.447
- 0.44
- 0.412
- 0.413
- 0.393
- 0.377
- 0.446
- 0.394
- 0.427
- 0.372
- 0.387
- 0.401
- 0.407
- 0.367
- 0.419
- 0.403
- 0.374
- 0.399
- 0.396
- 0.397
- 0.397
- 0.348
- 0.403
- 0.403
- 0.36
- 0.383
- 0.392
- 0.337
- 0.372
- 0.403
- 0.345
- 0.394
- 0.376
- 0.385
- 0.379
- 0.378
- 0.33
- 0.363
- 0.337
- 0.338
- 0.381
- 0.362
- 0.339
- 0.332
- 0.321
- 0.348
- 0.363
- 0.351
unequal: 0
verbose: 1
