avg_train_accuracy: 0.735
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.5
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 6
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.029627659574468085
- 0.0922340425531915
- 0.09398936170212766
- 0.1224468085106383
- 0.14675531914893616
- 0.17356382978723403
- 0.21015957446808511
- 0.16276595744680852
- 0.19872340425531915
- 0.13654255319148936
- 0.20079787234042554
- 0.17148936170212767
- 0.20132978723404255
- 0.24069148936170212
- 0.246968085106383
- 0.20308510638297872
- 0.23601063829787233
- 0.24425531914893617
- 0.266968085106383
- 0.18835106382978722
- 0.24664893617021277
- 0.2474468085106383
- 0.2851063829787234
- 0.215
- 0.26382978723404255
- 0.31170212765957445
- 0.2848936170212766
- 0.3377127659574468
- 0.31090425531914895
- 0.2701595744680851
- 0.28627659574468084
- 0.3403191489361702
- 0.3486170212765957
- 0.2771276595744681
- 0.2921808510638298
- 0.34319148936170213
- 0.3316489361702128
- 0.3029787234042553
- 0.29984042553191487
- 0.2928191489361702
- 0.4007978723404255
- 0.3321808510638298
- 0.3399468085106383
- 0.3254255319148936
- 0.33095744680851064
- 0.38888297872340427
- 0.24718085106382978
- 0.38888297872340427
- 0.3928191489361702
- 0.38601063829787235
- 0.3426595744680851
- 0.4197872340425532
- 0.4378723404255319
- 0.38377659574468087
- 0.4268085106382979
- 0.3984042553191489
- 0.3830851063829787
- 0.4375531914893617
- 0.35904255319148937
- 0.39478723404255317
- 0.4022340425531915
- 0.3772872340425532
- 0.4047872340425532
- 0.39159574468085107
- 0.27
- 0.27702127659574466
- 0.4226595744680851
- 0.3601063829787234
- 0.4472872340425532
- 0.454468085106383
- 0.3917553191489362
- 0.29808510638297875
- 0.43420212765957444
- 0.34377659574468084
- 0.44340425531914895
- 0.43920212765957445
- 0.4465425531914894
- 0.38648936170212767
- 0.4589893617021277
- 0.4455851063829787
- 0.38127659574468087
- 0.3122340425531915
- 0.4377659574468085
- 0.38648936170212767
- 0.36819148936170215
- 0.4296276595744681
- 0.4497872340425532
- 0.49186170212765956
- 0.43340425531914895
- 0.3223936170212766
- 0.47882978723404257
- 0.4469148936170213
- 0.44335106382978723
- 0.4324468085106383
- 0.3943617021276596
- 0.389468085106383
- 0.4735106382978723
- 0.3721808510638298
- 0.3891489361702128
- 0.44765957446808513
test_loss_list:
- 4.747009805043539
- 4.3115754954020185
- 3.9309353733062746
- 4.02002057393392
- 3.792767143249512
- 3.5909864934285483
- 3.442274519602458
- 3.3917199770609536
- 4.31354528427124
- 3.6009076245625815
- 3.8159893290201823
- 4.088878326416015
- 3.3289547634124754
- 3.2863123480478924
- 3.193746747970581
- 3.7451329453786215
- 3.37058832804362
- 3.63521169980367
- 3.099605518976847
- 4.03452130317688
- 3.5552752049764
- 3.404562946955363
- 3.020549046198527
- 4.7478692309061685
- 3.1782300980885823
- 3.1218550618489584
- 3.5316898314158123
- 3.062748982111613
- 3.2665361309051515
- 3.5967957401275634
- 3.6088474559783936
- 3.4607341925303143
- 3.034432112375895
- 3.55366818745931
- 3.185907627741496
- 3.2556459426879885
- 3.1374738089243572
- 3.4720053641001383
- 3.5628434467315673
- 3.945404726664225
- 3.0050838406880698
- 3.4089735062917073
- 3.037208261489868
- 3.3159423542022703
- 3.4402786986033123
- 2.9688125101725262
- 4.4590260060628255
- 2.850316063563029
- 2.9983253288269043
- 3.0385428365071614
- 3.5211512279510497
- 2.8425649992624917
- 2.8147126038869223
- 2.97469038327535
- 2.9826683394114175
- 3.232555767695109
- 3.4560160986582438
- 2.80761638323466
- 3.3191508547465007
- 3.1900097942352295
- 2.952106615702311
- 2.943841813405355
- 2.988497619628906
- 3.213433043162028
- 4.049988460540772
- 3.946254097620646
- 2.848053080240885
- 3.4738615194956464
- 2.9553948497772216
- 2.797192475001017
- 3.3233880170186363
- 4.071297998428345
- 2.8462518056233725
- 3.5738901774088543
- 2.8723423639933268
- 2.918222681681315
- 2.9472252559661865
- 3.208312225341797
- 2.833513158162435
- 3.045650768280029
- 3.2651450729370115
- 3.875193401972453
- 2.9049411042531332
- 3.0649188995361327
- 3.5308269850413003
- 3.0662786102294923
- 2.9556093311309812
- 2.7310996850331626
- 3.0781246693929036
- 4.032041041056315
- 2.9535880502065024
- 2.8083801873524985
- 3.0576364835103353
- 3.1547649065653482
- 3.154387486775716
- 3.301409470240275
- 2.7777157751719157
- 3.6750800164540607
- 3.4393348852793375
- 2.962326765060425
train_accuracy:
- 0.0
- 0.0
- 0.0
- 0.002
- 0.473
- 0.083
- 0.017
- 0.006
- 0.083
- 0.21
- 0.0
- 0.269
- 0.025
- 0.025
- 0.037
- 0.031
- 0.352
- 0.09
- 0.113
- 0.088
- 0.023
- 0.662
- 0.052
- 0.058
- 0.127
- 0.121
- 0.027
- 0.133
- 0.533
- 0.723
- 0.065
- 0.137
- 0.044
- 0.092
- 0.077
- 0.071
- 0.542
- 0.113
- 0.077
- 0.698
- 0.146
- 0.131
- 0.094
- 0.096
- 0.04
- 0.64
- 0.131
- 0.065
- 0.094
- 0.06
- 0.113
- 0.102
- 0.094
- 0.027
- 0.073
- 0.706
- 0.129
- 0.079
- 0.633
- 0.065
- 0.081
- 0.102
- 0.071
- 0.125
- 0.146
- 0.125
- 0.129
- 0.817
- 0.031
- 0.052
- 0.702
- 0.127
- 0.056
- 0.008
- 0.077
- 0.679
- 0.142
- 0.621
- 0.015
- 0.708
- 0.054
- 0.89
- 0.088
- 0.144
- 0.14
- 0.123
- 0.146
- 0.044
- 0.11
- 0.783
- 0.073
- 0.083
- 0.744
- 0.108
- 0.098
- 0.148
- 0.11
- 0.135
- 0.127
- 0.735
train_loss:
- 2.21
- 1.312
- 1.093
- 0.935
- 0.799
- 0.751
- 0.776
- 0.9
- 0.756
- 0.877
- 0.726
- 0.719
- 0.78
- 0.72
- 0.636
- 0.631
- 0.646
- 0.697
- 0.707
- 0.681
- 0.622
- 0.614
- 0.623
- 0.584
- 0.655
- 0.541
- 0.572
- 0.546
- 0.559
- 0.506
- 0.556
- 0.489
- 0.501
- 0.515
- 0.567
- 0.513
- 0.532
- 0.48
- 0.482
- 0.482
- 0.465
- 0.414
- 0.519
- 0.429
- 0.421
- 0.489
- 0.429
- 0.524
- 0.434
- 0.402
- 0.47
- 0.483
- 0.427
- 0.375
- 0.411
- 0.4
- 0.38
- 0.481
- 0.457
- 0.384
- 0.417
- 0.407
- 0.399
- 0.419
- 0.363
- 0.428
- 0.387
- 0.383
- 0.391
- 0.361
- 0.366
- 0.344
- 0.367
- 0.314
- 0.377
- 0.401
- 0.347
- 0.395
- 0.335
- 0.331
- 0.389
- 0.356
- 0.402
- 0.36
- 0.318
- 0.373
- 0.364
- 0.383
- 0.358
- 0.342
- 0.381
- 0.37
- 0.326
- 0.348
- 0.345
- 0.327
- 0.388
- 0.364
- 0.368
- 0.363
unequal: 0
verbose: 1
