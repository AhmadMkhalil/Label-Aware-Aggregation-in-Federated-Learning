avg_train_accuracy: 0.871
avg_train_loss: 0.006
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.3
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 4
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.05218085106382979
- 0.1922340425531915
- 0.26468085106382977
- 0.3192553191489362
- 0.329468085106383
- 0.34872340425531917
- 0.35904255319148937
- 0.37898936170212766
- 0.3949468085106383
- 0.4124468085106383
- 0.4007978723404255
- 0.4110106382978723
- 0.4302659574468085
- 0.4397340425531915
- 0.43117021276595746
- 0.4472872340425532
- 0.08441489361702127
- 0.15351063829787234
- 0.4468617021276596
- 0.4487765957446809
- 0.16835106382978723
- 0.45680851063829786
- 0.4648404255319149
- 0.20590425531914894
- 0.4703191489361702
- 0.46925531914893615
- 0.4729255319148936
- 0.4676063829787234
- 0.4739893617021277
- 0.4703191489361702
- 0.475
- 0.48058510638297874
- 0.48186170212765955
- 0.48127659574468085
- 0.47622340425531917
- 0.26090425531914896
- 0.48377659574468085
- 0.4845212765957447
- 0.48351063829787233
- 0.4851063829787234
- 0.4853723404255319
- 0.48414893617021276
- 0.4863297872340426
- 0.4870744680851064
- 0.48659574468085104
- 0.4899468085106383
- 0.49367021276595746
- 0.48946808510638296
- 0.490531914893617
- 0.4921276595744681
- 0.4950531914893617
- 0.49196808510638296
- 0.4922340425531915
- 0.49159574468085104
- 0.4965425531914894
- 0.4971808510638298
- 0.4981914893617021
- 0.49569148936170215
- 0.4945212765957447
- 0.2631914893617021
- 0.27047872340425533
- 0.49872340425531914
- 0.3599468085106383
- 0.5002659574468085
- 0.49925531914893617
- 0.5002127659574468
- 0.4996276595744681
- 0.5001063829787235
- 0.5013297872340425
- 0.5006914893617022
- 0.5033510638297872
- 0.5043617021276596
- 0.5023404255319149
- 0.49888297872340426
- 0.4999468085106383
- 0.5011702127659574
- 0.49829787234042555
- 0.4998936170212766
- 0.5012765957446809
- 0.5051595744680851
- 0.5096808510638298
- 0.5012765957446809
- 0.501968085106383
- 0.5035106382978723
- 0.5028723404255319
- 0.5087765957446808
- 0.5033510638297872
- 0.5024468085106383
- 0.5026063829787234
- 0.502127659574468
- 0.5053191489361702
- 0.5081382978723404
- 0.5053191489361702
- 0.5050531914893617
- 0.5075
- 0.5051063829787235
- 0.5064893617021277
- 0.3522872340425532
- 0.5132978723404256
- 0.5064893617021277
test_loss_list:
- 3.883277807235718
- 3.9235599136352537
- 3.763332535425822
- 3.816856616338094
- 3.5741568279266356
- 3.5217640876770018
- 3.6382528146107993
- 3.6652633921305338
- 4.014500153859457
- 4.11842503229777
- 3.6594157600402832
- 3.6125329526265464
- 3.9710184892018634
- 4.9018069330851235
- 3.543416862487793
- 4.303590237299601
- 4.453474871317545
- 4.048164939880371
- 3.3401054096221925
- 3.0584283860524497
- 3.5961576398213704
- 2.8687643082936605
- 3.9876115385691326
- 3.7283423137664795
- 2.7386512692769367
- 4.038258310953776
- 4.311403544743856
- 3.15336283047994
- 3.5347575092315675
- 3.0641964594523112
- 3.114276126225789
- 4.412056372960408
- 3.8225056489308673
- 3.6360132122039794
- 3.1894038581848143
- 2.936639617284139
- 3.339860375722249
- 3.4867398420969646
- 2.963868360519409
- 3.0582266680399575
- 3.3024282964070637
- 2.988364477157593
- 3.2729149627685548
- 3.395660123825073
- 2.9762563546498617
- 3.4995386250813802
- 4.12728941599528
- 3.1677938175201414
- 2.906886812845866
- 3.5759700552622475
- 4.258835805257162
- 3.54801061630249
- 3.1383042748769125
- 3.4421724319458007
- 4.287979389826456
- 3.539767338434855
- 3.602723108927409
- 3.6157537492116294
- 3.02626882870992
- 3.011112820307414
- 2.980771687825521
- 2.899222116470337
- 2.5066939862569173
- 2.8195050017038983
- 3.136160933176676
- 2.5894408003489175
- 3.120779962539673
- 2.737665236790975
- 3.111813942591349
- 2.7578469435373942
- 2.7847102610270182
- 2.6897337849934897
- 3.298092244466146
- 3.1613603528340657
- 3.9470607725779217
- 3.337234665552775
- 3.318719832102458
- 2.8312293243408204
- 3.162509667078654
- 3.2716015338897706
- 2.764764426549276
- 2.709777708053589
- 3.1768215243021647
- 3.2249097220102945
- 2.770714276631673
- 3.1606324195861815
- 3.9340637493133546
- 2.8003817303975422
- 3.298617458343506
- 3.1374190203348795
- 3.1979242451985677
- 2.7845157686869304
- 3.1780108801523843
- 2.826519702275594
- 2.647961753209432
- 3.7032337315877277
- 2.644133478800456
- 2.3372884019215903
- 2.8556235535939534
- 2.9177764447530112
train_accuracy:
- 0.079
- 0.327
- 0.452
- 0.575
- 0.0
- 0.0
- 0.0
- 0.679
- 0.725
- 0.0
- 0.0
- 0.0
- 0.758
- 0.796
- 0.812
- 0.0
- 0.0
- 0.45
- 0.783
- 0.006
- 0.796
- 0.0
- 0.852
- 0.033
- 0.029
- 0.829
- 0.829
- 0.825
- 0.84
- 0.012
- 0.0
- 0.84
- 0.85
- 0.844
- 0.0
- 0.54
- 0.844
- 0.844
- 0.01
- 0.002
- 0.0
- 0.002
- 0.0
- 0.0
- 0.006
- 0.0
- 0.867
- 0.002
- 0.0
- 0.867
- 0.875
- 0.0
- 0.004
- 0.0
- 0.873
- 0.002
- 0.873
- 0.0
- 0.898
- 0.204
- 0.646
- 0.844
- 0.594
- 0.848
- 0.906
- 0.031
- 0.879
- 0.844
- 0.012
- 0.867
- 0.002
- 0.029
- 0.887
- 0.885
- 0.86
- 0.0
- 0.86
- 0.004
- 0.883
- 0.887
- 0.019
- 0.0
- 0.0
- 0.004
- 0.879
- 0.906
- 0.902
- 0.0
- 0.0
- 0.0
- 0.892
- 0.006
- 0.0
- 0.885
- 0.0
- 0.879
- 0.01
- 0.846
- 0.0
- 0.871
train_loss:
- 3.107
- 3.419
- 2.466
- 2.048
- 1.415
- 1.296
- 1.298
- 1.173
- 1.364
- 1.449
- 1.042
- 1.046
- 1.359
- 1.469
- 1.006
- 1.176
- 0.897
- 0.625
- 1.304
- 0.814
- 0.687
- 0.97
- 1.3
- 0.636
- 0.766
- 1.236
- 1.185
- 0.791
- 1.029
- 0.743
- 0.682
- 1.102
- 0.897
- 0.968
- 0.8
- 0.66
- 0.963
- 0.866
- 0.74
- 0.732
- 0.843
- 0.647
- 0.813
- 0.783
- 0.683
- 0.839
- 0.947
- 0.745
- 0.676
- 0.785
- 0.905
- 0.844
- 0.7
- 0.747
- 0.884
- 0.817
- 0.795
- 0.826
- 0.618
- 0.612
- 0.474
- 0.78
- 0.469
- 0.778
- 0.709
- 0.613
- 0.727
- 0.624
- 0.752
- 0.558
- 0.677
- 0.583
- 0.726
- 0.715
- 0.813
- 0.758
- 0.674
- 0.635
- 0.674
- 0.647
- 0.568
- 0.671
- 0.68
- 0.72
- 0.552
- 0.642
- 0.775
- 0.573
- 0.734
- 0.645
- 0.72
- 0.653
- 0.637
- 0.607
- 0.514
- 0.763
- 0.599
- 0.488
- 0.726
- 0.625
unequal: 0
verbose: 1
