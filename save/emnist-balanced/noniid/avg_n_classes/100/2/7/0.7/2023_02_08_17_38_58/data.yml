avg_train_accuracy: 0.848
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.7
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.04074468085106383
- 0.1097872340425532
- 0.2076595744680851
- 0.2756382978723404
- 0.2840957446808511
- 0.34095744680851064
- 0.3703723404255319
- 0.39941489361702126
- 0.414468085106383
- 0.42531914893617023
- 0.4344148936170213
- 0.44382978723404254
- 0.4602127659574468
- 0.46611702127659577
- 0.45989361702127657
- 0.4626063829787234
- 0.4782446808510638
- 0.4815957446808511
- 0.48
- 0.4965425531914894
- 0.4768085106382979
- 0.496436170212766
- 0.5028723404255319
- 0.502127659574468
- 0.5116489361702128
- 0.5176063829787234
- 0.5131914893617021
- 0.5196808510638298
- 0.523031914893617
- 0.5216489361702128
- 0.5256914893617022
- 0.5245212765957447
- 0.5304787234042553
- 0.536595744680851
- 0.5320212765957447
- 0.5352659574468085
- 0.5387765957446808
- 0.5387765957446808
- 0.5380851063829787
- 0.5440425531914893
- 0.5381382978723405
- 0.5505851063829788
- 0.549468085106383
- 0.5446808510638298
- 0.5540957446808511
- 0.5535106382978724
- 0.5540425531914893
- 0.5538829787234043
- 0.5565425531914894
- 0.5535106382978724
- 0.5573404255319149
- 0.5576595744680851
- 0.5613297872340426
- 0.5598936170212766
- 0.5572340425531915
- 0.561063829787234
- 0.5614893617021277
- 0.5645212765957447
- 0.5662234042553191
- 0.5632446808510638
- 0.5637765957446809
- 0.5678191489361702
- 0.5698404255319149
- 0.5704255319148936
- 0.5673936170212766
- 0.5706382978723404
- 0.5702127659574469
- 0.5723404255319149
- 0.5707978723404256
- 0.5692021276595745
- 0.5740957446808511
- 0.5723404255319149
- 0.5707978723404256
- 0.574840425531915
- 0.5751595744680851
- 0.578404255319149
- 0.5745744680851064
- 0.5756914893617021
- 0.5773404255319149
- 0.5776595744680851
- 0.5798936170212766
- 0.5736702127659574
- 0.5799468085106383
- 0.5767021276595745
- 0.5763829787234043
- 0.5747872340425532
- 0.574840425531915
- 0.5771276595744681
- 0.5740957446808511
- 0.5756382978723404
- 0.5776063829787234
- 0.5793617021276596
- 0.5785638297872341
- 0.5853723404255319
- 0.5797340425531915
- 0.5838829787234042
- 0.5795212765957447
- 0.5813297872340426
- 0.5814893617021276
- 0.5778191489361703
test_loss_list:
- 3.7893048540751137
- 3.6779208278656004
- 3.4136279455820717
- 3.226954285303752
- 3.0466907119750974
- 3.0125464026133217
- 2.9535414791107177
- 3.010100673039754
- 3.0026514116923013
- 2.8665594037373863
- 2.8518853759765626
- 2.756048008600871
- 2.8809632428487144
- 2.8492211278279624
- 2.747627738316854
- 2.6448944759368898
- 2.7307084019978842
- 2.6922368017832436
- 2.573642031351725
- 2.7940972900390624
- 2.4692050488789876
- 2.5548971271514893
- 2.638671932220459
- 2.536789560317993
- 2.7119071102142334
- 2.652464354832967
- 2.551649891535441
- 2.549617665608724
- 2.5746405569712323
- 2.5136035919189452
- 2.4793888918558755
- 2.422093639373779
- 2.5880404249827067
- 2.5456108061472573
- 2.491504799524943
- 2.4890693124135335
- 2.437407859166463
- 2.4296650091807046
- 2.4254104900360107
- 2.4262359205881756
- 2.259286502202352
- 2.552504825592041
- 2.3788854837417603
- 2.351070187886556
- 2.5677641105651854
- 2.3491175603866576
- 2.386139203707377
- 2.326902634302775
- 2.308250128428141
- 2.2626400645573934
- 2.4160006999969483
- 2.3668600686391197
- 2.40943599541982
- 2.3350199206670124
- 2.24436284383138
- 2.2036142555872598
- 2.129014121691386
- 2.1880774974822996
- 2.2671810086568196
- 2.131883487701416
- 2.158301205635071
- 2.3769749704996745
- 2.2747026252746583
- 2.3511286846796673
- 2.1675111118952435
- 2.099932231903076
- 2.234351561864217
- 2.3620083395640057
- 2.1470255359013874
- 2.088171140352885
- 2.145519313812256
- 2.136248339017232
- 2.0965797185897825
- 2.0492781082789104
- 2.096964438756307
- 2.2808517980575562
- 2.031115345954895
- 2.0887541564305625
- 2.1854432249069213
- 2.1300254980723063
- 2.1673358424504596
- 2.064174610773722
- 2.1824344412485757
- 2.0206939077377317
- 2.128773403167725
- 1.995631580352783
- 2.0365416844685873
- 2.082128537495931
- 2.0267703326543174
- 2.116245067914327
- 1.915786452293396
- 2.026424808502197
- 1.9195838689804077
- 1.9193350346883138
- 1.9252434794108073
- 2.116977310180664
- 1.9042228984832763
- 1.9779988431930542
- 2.038955526351929
- 1.909459563891093
train_accuracy:
- 0.0
- 0.148
- 0.31
- 0.4
- 0.0
- 0.0
- 0.0
- 0.581
- 0.619
- 0.0
- 0.635
- 0.0
- 0.0
- 0.681
- 0.7
- 0.0
- 0.0
- 0.69
- 0.0
- 0.0
- 0.0
- 0.002
- 0.0
- 0.0
- 0.723
- 0.0
- 0.76
- 0.0
- 0.0
- 0.0
- 0.0
- 0.767
- 0.787
- 0.0
- 0.754
- 0.0
- 0.798
- 0.796
- 0.0
- 0.0
- 0.767
- 0.825
- 0.823
- 0.0
- 0.002
- 0.002
- 0.0
- 0.004
- 0.002
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.808
- 0.0
- 0.0
- 0.004
- 0.0
- 0.0
- 0.0
- 0.0
- 0.808
- 0.0
- 0.002
- 0.0
- 0.827
- 0.844
- 0.0
- 0.0
- 0.0
- 0.846
- 0.0
- 0.0
- 0.017
- 0.84
- 0.84
- 0.021
- 0.0
- 0.844
- 0.0
- 0.0
- 0.819
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.823
- 0.0
- 0.84
- 0.827
- 0.04
- 0.846
- 0.854
- 0.0
- 0.004
- 0.06
- 0.848
train_loss:
- 2.061
- 1.879
- 1.321
- 1.524
- 0.734
- 1.002
- 0.939
- 1.172
- 1.136
- 0.87
- 0.842
- 0.802
- 0.967
- 0.972
- 0.764
- 0.523
- 0.716
- 0.719
- 0.516
- 0.883
- 0.522
- 0.487
- 0.671
- 0.664
- 0.812
- 0.651
- 0.631
- 0.628
- 0.606
- 0.624
- 0.455
- 0.434
- 0.748
- 0.592
- 0.564
- 0.576
- 0.562
- 0.587
- 0.552
- 0.578
- 0.408
- 0.69
- 0.541
- 0.518
- 0.682
- 0.521
- 0.531
- 0.499
- 0.53
- 0.533
- 0.628
- 0.635
- 0.627
- 0.508
- 0.494
- 0.38
- 0.357
- 0.364
- 0.482
- 0.364
- 0.483
- 0.579
- 0.604
- 0.601
- 0.471
- 0.351
- 0.592
- 0.594
- 0.351
- 0.332
- 0.449
- 0.478
- 0.441
- 0.471
- 0.451
- 0.565
- 0.478
- 0.452
- 0.573
- 0.446
- 0.56
- 0.451
- 0.536
- 0.452
- 0.557
- 0.465
- 0.456
- 0.539
- 0.43
- 0.531
- 0.317
- 0.431
- 0.418
- 0.297
- 0.444
- 0.517
- 0.434
- 0.423
- 0.53
- 0.438
unequal: 0
verbose: 1
