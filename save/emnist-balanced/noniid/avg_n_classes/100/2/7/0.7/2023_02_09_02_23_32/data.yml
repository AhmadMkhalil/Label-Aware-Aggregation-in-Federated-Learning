avg_train_accuracy: 0.833
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.7
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.04127659574468085
- 0.0601063829787234
- 0.13787234042553193
- 0.2772872340425532
- 0.31920212765957445
- 0.35569148936170214
- 0.3895744680851064
- 0.3997340425531915
- 0.4229255319148936
- 0.42164893617021276
- 0.43946808510638297
- 0.4480851063829787
- 0.45228723404255317
- 0.46606382978723404
- 0.4702659574468085
- 0.47579787234042553
- 0.48329787234042554
- 0.47335106382978726
- 0.48132978723404257
- 0.4926595744680851
- 0.4952127659574468
- 0.4980851063829787
- 0.5048404255319149
- 0.5081914893617021
- 0.5098404255319149
- 0.511436170212766
- 0.5169148936170213
- 0.5166489361702128
- 0.518563829787234
- 0.5220744680851064
- 0.5291489361702127
- 0.53
- 0.5265425531914893
- 0.5355319148936171
- 0.5319148936170213
- 0.5347340425531915
- 0.5313297872340426
- 0.5414893617021277
- 0.5440425531914893
- 0.5418617021276596
- 0.5428723404255319
- 0.5430851063829787
- 0.5465425531914894
- 0.5478723404255319
- 0.5502127659574468
- 0.5544148936170212
- 0.5545212765957447
- 0.5493617021276596
- 0.5578723404255319
- 0.5554255319148936
- 0.5554255319148936
- 0.5558510638297872
- 0.5574468085106383
- 0.5592021276595744
- 0.5582978723404255
- 0.558404255319149
- 0.5615425531914894
- 0.5602659574468085
- 0.5625
- 0.5629787234042554
- 0.563404255319149
- 0.5639893617021277
- 0.5653191489361702
- 0.564627659574468
- 0.5664893617021277
- 0.5685106382978723
- 0.565
- 0.5668617021276596
- 0.5702659574468085
- 0.5715425531914894
- 0.5695212765957447
- 0.5720744680851064
- 0.5673404255319149
- 0.5685106382978723
- 0.5722340425531914
- 0.5737234042553192
- 0.5684574468085106
- 0.5755851063829788
- 0.571063829787234
- 0.5735106382978723
- 0.5742021276595745
- 0.5754787234042553
- 0.5775
- 0.5722340425531914
- 0.5762765957446808
- 0.5806914893617021
- 0.5787234042553191
- 0.5763829787234043
- 0.5780851063829787
- 0.5752659574468085
- 0.5778723404255319
- 0.5781382978723404
- 0.5757978723404256
- 0.5783510638297872
- 0.5795744680851064
- 0.5779255319148936
- 0.5793617021276596
- 0.5795744680851064
- 0.5788297872340425
- 0.5828191489361703
test_loss_list:
- 3.7985724035898842
- 3.7985603427886963
- 3.655308494567871
- 3.3882661215464274
- 3.171614087422689
- 3.021510041554769
- 3.019559462865194
- 2.8887218316396077
- 2.948304224014282
- 2.781577180226644
- 2.7588026269276935
- 2.758170216878255
- 2.693873701095581
- 2.72287872950236
- 2.705357103347778
- 2.6686144987742106
- 2.6611796379089356
- 2.606575043996175
- 2.516229995091756
- 2.574813664754232
- 2.5725911776224772
- 2.511300760904948
- 2.5668457317352296
- 2.54805580774943
- 2.3986318174997967
- 2.535327819188436
- 2.486394713719686
- 2.472908484141032
- 2.4452345021565756
- 2.5108377679189045
- 2.459926487604777
- 2.608142795562744
- 2.357836837768555
- 2.617800184885661
- 2.4055007139841718
- 2.4727835496266684
- 2.3339627885818484
- 2.515691515604655
- 2.5528190835316975
- 2.384328066507975
- 2.3886280743281048
- 2.4176926501592
- 2.5182671642303465
- 2.4814396063486734
- 2.3350671418507893
- 2.5058963616689045
- 2.5242676099141437
- 2.2241932566960654
- 2.557863025665283
- 2.3346769348780314
- 2.3856053733825684
- 2.458931622505188
- 2.4515017382303874
- 2.421177724202474
- 2.267087457974752
- 2.4246684249242145
- 2.260151712099711
- 2.3111154476801556
- 2.2843214893341064
- 2.2369603459040324
- 2.290504102706909
- 2.2079145749409994
- 2.231843946774801
- 2.2202089118957518
- 2.2158344570795694
- 2.198344276746114
- 2.1518063306808473
- 2.0677356719970703
- 2.1721348476409914
- 2.140876054763794
- 2.154814052581787
- 2.1054700819651284
- 2.090831386248271
- 2.0888106441497802
- 2.244302911758423
- 2.145921244621277
- 2.0236545530954997
- 2.074196000099182
- 2.0538477913538613
- 2.22280446211497
- 2.083459169069926
- 2.020275322596232
- 1.9215413649876913
- 1.9234379577636718
- 1.9300440009435018
- 2.133910757700602
- 1.98749636332194
- 2.0304878822962444
- 2.1513334035873415
- 2.1050315268834434
- 2.1227845509847003
- 2.0349242639541627
- 1.970100868542989
- 2.055638834635417
- 1.9887266969680786
- 1.9503900178273519
- 1.8478903659184773
- 1.836246549288432
- 1.9487763277689616
- 1.9001044495900472
train_accuracy:
- 0.0
- 0.077
- 0.0
- 0.0
- 0.0
- 0.0
- 0.579
- 0.0
- 0.0
- 0.0
- 0.0
- 0.617
- 0.006
- 0.0
- 0.008
- 0.0
- 0.0
- 0.0
- 0.698
- 0.0
- 0.0
- 0.0
- 0.0
- 0.685
- 0.681
- 0.683
- 0.0
- 0.0
- 0.0
- 0.012
- 0.0
- 0.0
- 0.0
- 0.0
- 0.006
- 0.0
- 0.048
- 0.0
- 0.0
- 0.0
- 0.0
- 0.76
- 0.767
- 0.76
- 0.0
- 0.0
- 0.0
- 0.767
- 0.0
- 0.806
- 0.819
- 0.0
- 0.781
- 0.0
- 0.0
- 0.046
- 0.8
- 0.781
- 0.812
- 0.0
- 0.0
- 0.0
- 0.079
- 0.0
- 0.0
- 0.071
- 0.0
- 0.006
- 0.0
- 0.0
- 0.0
- 0.798
- 0.0
- 0.0
- 0.835
- 0.0
- 0.0
- 0.002
- 0.831
- 0.077
- 0.0
- 0.0
- 0.0
- 0.019
- 0.0
- 0.0
- 0.01
- 0.0
- 0.802
- 0.835
- 0.798
- 0.802
- 0.002
- 0.0
- 0.002
- 0.0
- 0.808
- 0.0
- 0.09
- 0.833
train_loss:
- 1.667
- 1.971
- 1.441
- 1.273
- 1.095
- 0.979
- 1.232
- 0.87
- 1.116
- 0.829
- 0.598
- 0.773
- 0.542
- 0.717
- 0.733
- 0.708
- 0.704
- 0.493
- 0.486
- 0.648
- 0.622
- 0.468
- 0.63
- 0.614
- 0.459
- 0.585
- 0.588
- 0.577
- 0.562
- 0.571
- 0.582
- 0.698
- 0.417
- 0.693
- 0.554
- 0.546
- 0.393
- 0.665
- 0.67
- 0.531
- 0.519
- 0.506
- 0.641
- 0.63
- 0.523
- 0.624
- 0.621
- 0.38
- 0.618
- 0.499
- 0.481
- 0.61
- 0.604
- 0.596
- 0.509
- 0.594
- 0.479
- 0.471
- 0.459
- 0.47
- 0.465
- 0.462
- 0.458
- 0.458
- 0.436
- 0.437
- 0.456
- 0.324
- 0.453
- 0.438
- 0.428
- 0.433
- 0.318
- 0.433
- 0.54
- 0.44
- 0.316
- 0.438
- 0.414
- 0.522
- 0.424
- 0.305
- 0.331
- 0.304
- 0.314
- 0.529
- 0.411
- 0.409
- 0.518
- 0.516
- 0.531
- 0.405
- 0.309
- 0.52
- 0.414
- 0.411
- 0.313
- 0.299
- 0.404
- 0.412
unequal: 0
verbose: 1
