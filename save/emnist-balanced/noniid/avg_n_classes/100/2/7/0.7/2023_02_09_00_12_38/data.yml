avg_train_accuracy: 0.0
avg_train_loss: 0.005
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.7
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.028297872340425533
- 0.07851063829787235
- 0.21319148936170212
- 0.30686170212765956
- 0.34180851063829787
- 0.37946808510638297
- 0.40680851063829787
- 0.40835106382978725
- 0.4299468085106383
- 0.44393617021276593
- 0.448563829787234
- 0.4538829787234043
- 0.45776595744680854
- 0.46702127659574466
- 0.47510638297872343
- 0.4803723404255319
- 0.48425531914893616
- 0.48925531914893616
- 0.49946808510638296
- 0.4997872340425532
- 0.4973936170212766
- 0.5055851063829787
- 0.501436170212766
- 0.5143617021276595
- 0.5182978723404256
- 0.508563829787234
- 0.5190425531914894
- 0.5151063829787234
- 0.518563829787234
- 0.5261170212765958
- 0.5318617021276596
- 0.5376063829787234
- 0.5285106382978724
- 0.5370744680851064
- 0.5344148936170213
- 0.5370212765957447
- 0.5398404255319149
- 0.5448404255319149
- 0.5460106382978723
- 0.5484042553191489
- 0.5498936170212766
- 0.5481914893617021
- 0.5514361702127659
- 0.546595744680851
- 0.5529255319148936
- 0.5584574468085106
- 0.5509574468085107
- 0.5582978723404255
- 0.5537234042553192
- 0.5576595744680851
- 0.5593085106382979
- 0.5623404255319149
- 0.560531914893617
- 0.5606914893617021
- 0.5613297872340426
- 0.5636170212765957
- 0.5648404255319149
- 0.5626063829787235
- 0.5686170212765957
- 0.5674468085106383
- 0.5655851063829788
- 0.5679787234042554
- 0.5680851063829787
- 0.5657446808510638
- 0.5695744680851064
- 0.5704255319148936
- 0.5771808510638298
- 0.5748936170212766
- 0.5730851063829787
- 0.5717553191489362
- 0.5716489361702127
- 0.573936170212766
- 0.574627659574468
- 0.5740957446808511
- 0.5762234042553191
- 0.5757446808510638
- 0.5770744680851064
- 0.5755851063829788
- 0.5770744680851064
- 0.5781914893617022
- 0.5811170212765957
- 0.5770212765957446
- 0.5787765957446809
- 0.5798936170212766
- 0.5801595744680851
- 0.5787234042553191
- 0.581063829787234
- 0.5821808510638298
- 0.5818085106382979
- 0.5829787234042553
- 0.5811170212765957
- 0.5820212765957447
- 0.5827127659574468
- 0.5815425531914894
- 0.5852659574468085
- 0.5877127659574468
- 0.5865957446808511
- 0.5848404255319148
- 0.5827659574468085
- 0.5839893617021277
test_loss_list:
- 3.790600299835205
- 3.7419457403818766
- 3.538436180750529
- 3.301289618810018
- 3.106767272949219
- 3.074953908920288
- 2.9947458934783935
- 2.883932730356852
- 2.9306609853108725
- 3.001458641688029
- 2.9247473367055257
- 2.9373605473836264
- 2.8591740067799885
- 2.949424336751302
- 2.929659252166748
- 2.856517168680827
- 2.8045250352223716
- 2.8527445729573566
- 2.9276623662312824
- 2.8402819442749023
- 2.6941277313232423
- 2.7778946336110435
- 2.5386446444193522
- 2.717799164454142
- 2.760564432144165
- 2.5309157530466715
- 2.603284991582235
- 2.484302218755086
- 2.5122405020395915
- 2.5309276072184246
- 2.521008710861206
- 2.66034228960673
- 2.420217717488607
- 2.5500573921203613
- 2.3856844011942546
- 2.4055923080444335
- 2.4117738978068033
- 2.6016929562886557
- 2.434988702138265
- 2.465634902318319
- 2.556752160390218
- 2.3907199700673423
- 2.4048086992899576
- 2.229312326113383
- 2.340663007100423
- 2.4663419119517007
- 2.272933260599772
- 2.4756243228912354
- 2.2919319597880046
- 2.2738373915354413
- 2.358512355486552
- 2.4277347564697265
- 2.4414357566833496
- 2.293770098686218
- 2.440873524347941
- 2.328582461675008
- 2.4389688460032146
- 2.312522662480672
- 2.2516589260101316
- 2.2573151620229086
- 2.11741805712382
- 2.245248203277588
- 2.2416894912719725
- 2.192921975453695
- 2.248674306869507
- 2.1930556758244832
- 2.199497097333272
- 2.1873924493789674
- 2.1159385220209757
- 2.065310076077779
- 2.28443816502889
- 2.10290212949117
- 2.2421632210413613
- 2.1350211842854816
- 2.3330930916468304
- 2.14280598004659
- 2.1617081038157147
- 2.2025362777709963
- 2.1169960419336955
- 2.096587158838908
- 2.11021372795105
- 2.0467744557062786
- 2.0777011108398438
- 1.94866690158844
- 2.069858104387919
- 2.1742843341827394
- 2.053910007476807
- 2.054679225285848
- 2.1652880160013837
- 2.0824266974131267
- 1.9371116574605305
- 2.056869815190633
- 1.922242538134257
- 2.058917586008708
- 1.87395006497701
- 1.8173490571975708
- 1.9773124488194784
- 1.9357739766438802
- 1.9863171482086182
- 2.1180377356211344
train_accuracy:
- 0.0
- 0.088
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.656
- 0.594
- 0.0
- 0.0
- 0.0
- 0.665
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.679
- 0.0
- 0.0
- 0.0
- 0.0
- 0.688
- 0.0
- 0.771
- 0.002
- 0.0
- 0.767
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.735
- 0.75
- 0.0
- 0.0
- 0.0
- 0.0
- 0.783
- 0.827
- 0.838
- 0.0
- 0.0
- 0.806
- 0.0
- 0.0
- 0.0
- 0.0
- 0.833
- 0.0
- 0.0
- 0.0
- 0.002
- 0.781
- 0.0
- 0.771
- 0.852
- 0.808
- 0.002
- 0.0
- 0.0
- 0.0
- 0.846
- 0.0
- 0.0
- 0.0
- 0.852
- 0.004
- 0.002
- 0.0
- 0.8
- 0.0
- 0.0
- 0.0
- 0.0
- 0.002
- 0.0
- 0.775
- 0.787
- 0.0
- 0.0
train_loss:
- 1.202
- 1.455
- 1.324
- 1.169
- 0.701
- 0.969
- 0.881
- 0.606
- 0.869
- 1.079
- 0.82
- 0.755
- 0.736
- 0.916
- 0.946
- 0.744
- 0.704
- 0.86
- 0.83
- 0.862
- 0.666
- 0.642
- 0.493
- 0.59
- 0.76
- 0.457
- 0.457
- 0.444
- 0.431
- 0.591
- 0.59
- 0.692
- 0.429
- 0.559
- 0.414
- 0.404
- 0.536
- 0.648
- 0.51
- 0.512
- 0.649
- 0.531
- 0.5
- 0.395
- 0.518
- 0.646
- 0.378
- 0.634
- 0.364
- 0.49
- 0.475
- 0.595
- 0.628
- 0.474
- 0.605
- 0.454
- 0.567
- 0.478
- 0.484
- 0.457
- 0.36
- 0.467
- 0.446
- 0.465
- 0.431
- 0.464
- 0.457
- 0.459
- 0.339
- 0.333
- 0.562
- 0.452
- 0.416
- 0.438
- 0.547
- 0.437
- 0.408
- 0.553
- 0.444
- 0.441
- 0.408
- 0.425
- 0.406
- 0.327
- 0.425
- 0.508
- 0.398
- 0.423
- 0.508
- 0.395
- 0.32
- 0.409
- 0.307
- 0.386
- 0.312
- 0.302
- 0.402
- 0.414
- 0.402
- 0.493
unequal: 0
verbose: 1
