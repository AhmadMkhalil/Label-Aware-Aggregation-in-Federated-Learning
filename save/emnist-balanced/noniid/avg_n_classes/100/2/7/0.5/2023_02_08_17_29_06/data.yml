avg_train_accuracy: 0.85
avg_train_loss: 0.005
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.5
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.03297872340425532
- 0.13122340425531914
- 0.2075
- 0.31127659574468086
- 0.36388297872340425
- 0.375
- 0.11851063829787234
- 0.4128723404255319
- 0.43446808510638296
- 0.44590425531914896
- 0.4598404255319149
- 0.4651595744680851
- 0.4833510638297872
- 0.1976063829787234
- 0.4749468085106383
- 0.4832446808510638
- 0.4984574468085106
- 0.505531914893617
- 0.5056382978723404
- 0.5107446808510638
- 0.5017021276595744
- 0.5203191489361703
- 0.5201595744680851
- 0.5216489361702128
- 0.5267021276595745
- 0.5268617021276596
- 0.2954787234042553
- 0.21691489361702126
- 0.5312234042553191
- 0.533031914893617
- 0.5327127659574468
- 0.5393085106382979
- 0.5434574468085106
- 0.5359574468085107
- 0.30781914893617024
- 0.5394148936170213
- 0.5384042553191489
- 0.5500531914893617
- 0.5483510638297873
- 0.3524468085106383
- 0.5512234042553191
- 0.40164893617021274
- 0.5522340425531915
- 0.5472340425531915
- 0.5485638297872341
- 0.5533510638297873
- 0.5571276595744681
- 0.5574468085106383
- 0.4506914893617021
- 0.5542021276595744
- 0.559468085106383
- 0.556063829787234
- 0.5576063829787234
- 0.5596808510638298
- 0.5572872340425532
- 0.5618085106382978
- 0.5621808510638298
- 0.5661170212765958
- 0.5623936170212765
- 0.5672872340425532
- 0.5662765957446808
- 0.5663829787234043
- 0.566968085106383
- 0.5653723404255319
- 0.5666489361702127
- 0.5667553191489362
- 0.5690957446808511
- 0.5693617021276596
- 0.5127127659574469
- 0.5751595744680851
- 0.5695744680851064
- 0.57
- 0.5711702127659575
- 0.4970212765957447
- 0.5746808510638298
- 0.575
- 0.5762234042553191
- 0.5696808510638298
- 0.5716489361702127
- 0.5711702127659575
- 0.573031914893617
- 0.5714361702127659
- 0.571968085106383
- 0.573404255319149
- 0.5735106382978723
- 0.5680851063829787
- 0.5747872340425532
- 0.5348936170212766
- 0.584095744680851
- 0.5764893617021276
- 0.5744148936170212
- 0.573404255319149
- 0.576968085106383
- 0.5751063829787234
- 0.5813297872340426
- 0.5067021276595745
- 0.5824468085106383
- 0.5842553191489361
- 0.5816489361702127
- 0.5794680851063829
test_loss_list:
- 3.7640997219085692
- 3.7012161668141683
- 3.4440908241271972
- 3.205588213602702
- 3.133200511932373
- 3.0566392707824708
- 3.864129581451416
- 2.9083323860168457
- 2.909226868947347
- 2.957617400487264
- 2.9193983332316082
- 2.8963844998677573
- 3.049375187555949
- 3.340761597951253
- 2.6555603694915773
- 2.6926899337768555
- 2.77860143661499
- 2.8967254956563315
- 2.8740348466237386
- 2.7566555754343667
- 2.7696474488576253
- 2.7830091190338133
- 2.811147928237915
- 2.8619841289520265
- 2.837840789159139
- 2.7281356716156004
- 2.6361331780751547
- 3.295884517033895
- 2.359981606801351
- 2.5034614721934
- 2.50564910252889
- 2.5935422579447427
- 2.7566538206736246
- 2.558871742884318
- 2.5526606051127114
- 2.282326065699259
- 2.417482018470764
- 2.5918838278452556
- 2.527081470489502
- 2.304120891888936
- 2.314599315325419
- 2.160702695846558
- 2.1300530672073363
- 2.2474972947438556
- 2.275736926396688
- 2.333655325571696
- 2.4564749956130982
- 2.287674930890401
- 1.9540585374832153
- 2.0825446208318072
- 2.2176837666829425
- 2.207355783780416
- 2.215239303906759
- 2.242108615239461
- 2.2902249002456667
- 2.419858218828837
- 2.359819588661194
- 2.608463697433472
- 2.4318205451965333
- 2.40726101398468
- 2.4187596400578815
- 2.3704787158966063
- 2.473452033996582
- 2.416236743927002
- 2.2401789315541585
- 2.270531822840373
- 2.3322818835576373
- 2.4855883916219077
- 1.6357265090942383
- 2.0009647528330485
- 2.157728509902954
- 2.2221424547831217
- 2.251452159881592
- 1.6843023029963176
- 2.0864344183603922
- 2.1579307476679483
- 2.2512481037775673
- 2.1670317459106445
- 2.2820921468734743
- 2.2196877225240073
- 2.295266235669454
- 2.1663793325424194
- 2.0829993708928427
- 2.268867813746134
- 2.1278229633967083
- 2.1190983692804974
- 2.423090877532959
- 1.4817062060038249
- 1.8622011852264404
- 2.1999854882558187
- 1.9206111081441244
- 2.0211365365982057
- 2.173071354230245
- 2.2461039272944134
- 2.162235517501831
- 1.643698434829712
- 1.9540310875574747
- 1.9089103412628174
- 2.138151157697042
- 2.171442124048869
train_accuracy:
- 0.0
- 0.0
- 0.0
- 0.452
- 0.0
- 0.0
- 0.323
- 0.581
- 0.61
- 0.0
- 0.0
- 0.646
- 0.675
- 0.658
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.737
- 0.698
- 0.0
- 0.0
- 0.721
- 0.0
- 0.0
- 0.61
- 0.49
- 0.004
- 0.0
- 0.0
- 0.0
- 0.767
- 0.0
- 0.375
- 0.779
- 0.0
- 0.008
- 0.769
- 0.64
- 0.796
- 0.64
- 0.0
- 0.763
- 0.056
- 0.781
- 0.037
- 0.062
- 0.779
- 0.002
- 0.792
- 0.0
- 0.002
- 0.04
- 0.783
- 0.035
- 0.0
- 0.817
- 0.794
- 0.821
- 0.027
- 0.056
- 0.0
- 0.8
- 0.0
- 0.0
- 0.0
- 0.796
- 0.688
- 0.169
- 0.804
- 0.0
- 0.806
- 0.812
- 0.815
- 0.06
- 0.8
- 0.0
- 0.0
- 0.792
- 0.029
- 0.0
- 0.0
- 0.802
- 0.012
- 0.0
- 0.808
- 0.683
- 0.002
- 0.842
- 0.002
- 0.01
- 0.0
- 0.808
- 0.002
- 0.827
- 0.815
- 0.188
- 0.002
- 0.85
train_loss:
- 1.335
- 1.832
- 1.106
- 1.458
- 1.355
- 0.825
- 0.428
- 1.13
- 1.083
- 1.051
- 1.003
- 0.687
- 1.24
- 0.397
- 0.6
- 0.572
- 0.849
- 1.095
- 0.868
- 0.829
- 0.568
- 0.795
- 0.776
- 0.743
- 0.743
- 0.599
- 0.368
- 0.22
- 0.454
- 0.508
- 0.495
- 0.679
- 0.906
- 0.518
- 0.288
- 0.475
- 0.452
- 0.67
- 0.694
- 0.292
- 0.662
- 0.229
- 0.401
- 0.423
- 0.444
- 0.456
- 0.614
- 0.443
- 0.25
- 0.399
- 0.44
- 0.416
- 0.455
- 0.428
- 0.402
- 0.609
- 0.625
- 0.763
- 0.596
- 0.609
- 0.608
- 0.4
- 0.61
- 0.573
- 0.425
- 0.399
- 0.613
- 0.733
- 0.256
- 0.383
- 0.566
- 0.587
- 0.562
- 0.234
- 0.536
- 0.559
- 0.565
- 0.39
- 0.524
- 0.552
- 0.533
- 0.382
- 0.401
- 0.535
- 0.376
- 0.39
- 0.686
- 0.268
- 0.336
- 0.664
- 0.382
- 0.397
- 0.525
- 0.675
- 0.527
- 0.219
- 0.484
- 0.35
- 0.481
- 0.503
unequal: 0
verbose: 1
