avg_train_accuracy: 0.046
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.5
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.05095744680851064
- 0.14702127659574468
- 0.2651063829787234
- 0.3271276595744681
- 0.3496808510638298
- 0.3723936170212766
- 0.4006382978723404
- 0.411968085106383
- 0.4356914893617021
- 0.4313297872340425
- 0.44765957446808513
- 0.18601063829787234
- 0.4538829787234043
- 0.45
- 0.4726063829787234
- 0.47074468085106386
- 0.475531914893617
- 0.47760638297872343
- 0.4871276595744681
- 0.4873936170212766
- 0.4944148936170213
- 0.498936170212766
- 0.5061702127659574
- 0.49888297872340426
- 0.5030851063829788
- 0.5165425531914893
- 0.5228191489361702
- 0.5195744680851064
- 0.5245212765957447
- 0.5217553191489361
- 0.528031914893617
- 0.5384042553191489
- 0.541063829787234
- 0.3406914893617021
- 0.5370744680851064
- 0.5451063829787234
- 0.5346276595744681
- 0.5435106382978724
- 0.2549468085106383
- 0.5398936170212766
- 0.5463297872340426
- 0.4176595744680851
- 0.5530851063829787
- 0.5509574468085107
- 0.5480851063829787
- 0.5565425531914894
- 0.5493085106382979
- 0.5526595744680851
- 0.5542021276595744
- 0.5614361702127659
- 0.5639893617021277
- 0.5595212765957447
- 0.558031914893617
- 0.5635638297872341
- 0.5612234042553191
- 0.5647340425531915
- 0.3147872340425532
- 0.5673936170212766
- 0.37601063829787235
- 0.5696808510638298
- 0.5680851063829787
- 0.5658510638297872
- 0.5694148936170212
- 0.5712234042553191
- 0.5678723404255319
- 0.5686702127659574
- 0.5699468085106383
- 0.4299468085106383
- 0.5728723404255319
- 0.5739893617021277
- 0.5756382978723404
- 0.5757446808510638
- 0.5724468085106383
- 0.5756914893617021
- 0.49351063829787234
- 0.5752659574468085
- 0.5727659574468085
- 0.579095744680851
- 0.5763297872340426
- 0.5738829787234042
- 0.5788297872340425
- 0.5745744680851064
- 0.5747872340425532
- 0.5770212765957446
- 0.5785106382978723
- 0.576063829787234
- 0.5795744680851064
- 0.5777127659574468
- 0.5805851063829788
- 0.5338297872340425
- 0.5820212765957447
- 0.581968085106383
- 0.5358510638297872
- 0.586063829787234
- 0.5833510638297872
- 0.5842021276595745
- 0.5811702127659575
- 0.5835638297872341
- 0.5796808510638298
- 0.579095744680851
test_loss_list:
- 3.7744723002115887
- 3.6839045397440593
- 3.4406673113505044
- 3.2870403353373208
- 3.1647218577067058
- 3.0539724731445315
- 3.0736758422851564
- 3.099692548116048
- 3.21153626759847
- 3.0219369665781657
- 3.0414295132954914
- 3.143655185699463
- 2.7901577599843344
- 2.8408925120035806
- 3.0048178100585936
- 2.909495480855306
- 2.8783690611521404
- 2.8113850084940593
- 2.816068344116211
- 2.787821960449219
- 2.8011527633666993
- 2.8792333221435547
- 2.817429164250692
- 2.6917057609558106
- 2.7940154234568277
- 2.7841173934936525
- 2.8599819882710773
- 2.7938399092356363
- 2.752724984486898
- 2.7120424111684165
- 2.654733247756958
- 2.881203778584798
- 2.8312880833943685
- 2.3725394121805827
- 2.5398448117574057
- 2.861437800725301
- 2.59220308303833
- 2.8224785391489666
- 2.9612934430440268
- 2.358389983177185
- 2.4938658142089842
- 2.052200241088867
- 2.3544190295537315
- 2.3218678188323976
- 2.416219434738159
- 2.6728226884206134
- 2.2888273191452027
- 2.4104842535654702
- 2.541655937830607
- 2.5635104401906332
- 2.738740469614665
- 2.5890255324045817
- 2.6065852578481037
- 2.4645426495869955
- 2.3718132543563843
- 2.554439614613851
- 2.5821374225616456
- 2.20828914642334
- 2.152593218485514
- 2.246219464937846
- 2.315604174931844
- 2.2543310276667277
- 2.304869041442871
- 2.37118310769399
- 2.372753930091858
- 2.3090043528874715
- 2.260075910886129
- 1.9845683574676514
- 2.1518759806950887
- 2.259119472503662
- 2.2763472827275595
- 2.4816270224253336
- 2.3060234626134237
- 2.4935785865783693
- 1.761025120417277
- 2.022891583442688
- 2.1040963697433472
- 2.3060979557037355
- 2.2459551429748537
- 2.1876744492848714
- 2.24290301322937
- 2.1662044938405356
- 2.213153421084086
- 2.1911689058939614
- 2.435369749069214
- 2.164529555638631
- 2.386689411799113
- 2.262571409543355
- 2.311446647644043
- 1.592348386446635
- 1.9789298184712727
- 2.1292292642593384
- 1.588981006940206
- 1.7967379665374756
- 2.1601008574167886
- 2.006680655479431
- 2.0613330443700155
- 2.144749182065328
- 2.163847386042277
- 2.080366147359212
train_accuracy:
- 0.0
- 0.183
- 0.348
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.588
- 0.0
- 0.0
- 0.575
- 0.004
- 0.0
- 0.646
- 0.0
- 0.631
- 0.002
- 0.0
- 0.631
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.7
- 0.706
- 0.698
- 0.0
- 0.0
- 0.0
- 0.0
- 0.746
- 0.448
- 0.706
- 0.744
- 0.0
- 0.0
- 0.475
- 0.0
- 0.0
- 0.856
- 0.088
- 0.0
- 0.729
- 0.767
- 0.777
- 0.0
- 0.775
- 0.0
- 0.002
- 0.76
- 0.0
- 0.0
- 0.752
- 0.0
- 0.533
- 0.0
- 0.169
- 0.783
- 0.0
- 0.763
- 0.0
- 0.773
- 0.0
- 0.0
- 0.0
- 0.744
- 0.0
- 0.827
- 0.0
- 0.819
- 0.0
- 0.781
- 0.583
- 0.021
- 0.056
- 0.0
- 0.8
- 0.0
- 0.029
- 0.0
- 0.0
- 0.0
- 0.8
- 0.0
- 0.808
- 0.0
- 0.002
- 0.892
- 0.806
- 0.785
- 0.929
- 0.21
- 0.844
- 0.781
- 0.798
- 0.023
- 0.8
- 0.046
train_loss:
- 1.969
- 1.761
- 1.608
- 1.41
- 0.9
- 0.898
- 1.158
- 1.105
- 1.367
- 1.036
- 1.034
- 0.416
- 0.942
- 0.646
- 1.2
- 0.616
- 0.575
- 0.673
- 0.918
- 0.609
- 0.638
- 0.809
- 0.559
- 0.599
- 0.549
- 0.542
- 0.781
- 0.759
- 0.56
- 0.525
- 0.545
- 0.679
- 0.688
- 0.369
- 0.727
- 0.849
- 0.495
- 0.65
- 0.325
- 0.418
- 0.676
- 0.271
- 0.66
- 0.456
- 0.638
- 0.789
- 0.484
- 0.435
- 0.596
- 0.618
- 0.765
- 0.595
- 0.608
- 0.463
- 0.469
- 0.603
- 0.283
- 0.369
- 0.256
- 0.778
- 0.568
- 0.415
- 0.578
- 0.57
- 0.578
- 0.409
- 0.412
- 0.269
- 0.567
- 0.55
- 0.547
- 0.693
- 0.569
- 0.687
- 0.288
- 0.356
- 0.38
- 0.697
- 0.537
- 0.402
- 0.528
- 0.387
- 0.557
- 0.397
- 0.672
- 0.41
- 0.656
- 0.532
- 0.522
- 0.238
- 0.544
- 0.494
- 0.223
- 0.34
- 0.634
- 0.372
- 0.538
- 0.504
- 0.506
- 0.367
unequal: 0
verbose: 1
