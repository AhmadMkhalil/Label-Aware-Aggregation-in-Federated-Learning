avg_train_accuracy: 0.0
avg_train_loss: 0.004
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.5
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.046914893617021274
- 0.08696808510638297
- 0.22367021276595744
- 0.28106382978723404
- 0.3180851063829787
- 0.35398936170212764
- 0.38159574468085106
- 0.3982978723404255
- 0.4194148936170213
- 0.43186170212765956
- 0.44340425531914895
- 0.43457446808510636
- 0.4525
- 0.4605851063829787
- 0.4635106382978723
- 0.4703191489361702
- 0.4727127659574468
- 0.4807446808510638
- 0.4849468085106383
- 0.4915425531914894
- 0.4984574468085106
- 0.5020744680851064
- 0.5000531914893617
- 0.49792553191489364
- 0.1247340425531915
- 0.5085106382978724
- 0.5135106382978724
- 0.5115957446808511
- 0.5086170212765957
- 0.5206914893617022
- 0.11819148936170212
- 0.5146276595744681
- 0.5263297872340426
- 0.17313829787234042
- 0.5275531914893618
- 0.5322340425531915
- 0.5304787234042553
- 0.5333510638297873
- 0.3071276595744681
- 0.5357978723404255
- 0.5384574468085106
- 0.5388829787234043
- 0.5427659574468086
- 0.5438829787234043
- 0.5448404255319149
- 0.5451595744680852
- 0.5498936170212766
- 0.3382978723404255
- 0.5493085106382979
- 0.5477127659574468
- 0.5533510638297873
- 0.5543617021276596
- 0.5538297872340425
- 0.5573936170212765
- 0.5568085106382978
- 0.5567553191489362
- 0.5575531914893617
- 0.5567021276595745
- 0.5617021276595745
- 0.5611170212765958
- 0.5630851063829787
- 0.5612765957446808
- 0.5662765957446808
- 0.5653723404255319
- 0.5614361702127659
- 0.5672340425531915
- 0.5637234042553192
- 0.5656382978723404
- 0.5704787234042553
- 0.5721276595744681
- 0.5662234042553191
- 0.5715425531914894
- 0.25292553191489364
- 0.5715957446808511
- 0.5698404255319149
- 0.5677659574468085
- 0.5731914893617022
- 0.570531914893617
- 0.5723936170212766
- 0.5753723404255319
- 0.5728723404255319
- 0.5752659574468085
- 0.574840425531915
- 0.5762765957446808
- 0.5745744680851064
- 0.5792553191489361
- 0.5761170212765957
- 0.5765425531914894
- 0.5771808510638298
- 0.4448404255319149
- 0.5766489361702127
- 0.578404255319149
- 0.5781914893617022
- 0.5786170212765958
- 0.5812765957446808
- 0.5805851063829788
- 0.5817553191489362
- 0.5815957446808511
- 0.5827659574468085
- 0.5785106382978723
test_loss_list:
- 3.7945243072509767
- 3.758795836766561
- 3.550270938873291
- 3.374047193527222
- 3.259498027165731
- 3.1520340315500897
- 3.137458620071411
- 3.1683314768473307
- 3.112849496205648
- 3.135488516489665
- 3.2535251267751057
- 2.9605573749542238
- 3.1058176136016846
- 3.1584149328867595
- 2.874628791809082
- 3.0599393049875894
- 2.951469093958537
- 3.064524002075195
- 3.04545480410258
- 3.0680192216237385
- 3.172642787297567
- 3.199024346669515
- 2.9505143928527833
- 2.810279401143392
- 3.826701749165853
- 2.6899703184763593
- 2.8348084926605224
- 2.8741444746653237
- 2.7422551123301186
- 2.84965638478597
- 3.9275375175476075
- 2.4485606511433917
- 2.8074056816101076
- 3.577961238225301
- 2.3302246888478595
- 2.7311621475219727
- 2.5541475009918213
- 2.551915661493937
- 2.379889637629191
- 2.467998523712158
- 2.4090688896179198
- 2.4661963399251303
- 2.6036160151163736
- 2.4270818424224854
- 2.4837766297658286
- 2.479966163635254
- 2.591437129974365
- 2.356347812016805
- 2.321833480199178
- 2.3611191368103026
- 2.3101918013890583
- 2.535992908477783
- 2.3315729745229086
- 2.4195677185058595
- 2.507988344828288
- 2.510684356689453
- 2.478628485997518
- 2.379389522870382
- 2.314079990386963
- 2.5186823081970213
- 2.448935721715291
- 2.461710917154948
- 2.306921445528666
- 2.319242828687032
- 2.416311610539754
- 2.4131064430872597
- 2.3303920141855876
- 2.480038417180379
- 2.4667365646362303
- 2.439189540545146
- 2.315179829597473
- 2.281533708572388
- 2.8253628571828204
- 2.215608448982239
- 2.3494280274709065
- 2.267823387781779
- 2.304907565116882
- 2.24021489461263
- 2.3085806226730345
- 2.315716896057129
- 2.2360524495442706
- 2.4714023526509603
- 2.2349530839920044
- 2.326236793200175
- 2.408846559524536
- 2.487666362126668
- 2.2126617749532063
- 2.275717562039693
- 2.3169209480285646
- 1.89738707224528
- 2.029251608848572
- 2.1314243523279828
- 2.010379185676575
- 2.0772719033559164
- 2.2091555229822792
- 2.141121244430542
- 2.0764477109909056
- 2.177967457771301
- 2.059478635787964
- 2.134698813756307
train_accuracy:
- 0.054
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.635
- 0.0
- 0.0
- 0.0
- 0.665
- 0.7
- 0.7
- 0.0
- 0.0
- 0.0
- 0.713
- 0.706
- 0.0
- 0.75
- 0.0
- 0.0
- 0.758
- 0.0
- 0.0
- 0.825
- 0.0
- 0.74
- 0.669
- 0.0
- 0.0
- 0.0
- 0.0
- 0.81
- 0.0
- 0.0
- 0.0
- 0.802
- 0.0
- 0.794
- 0.79
- 0.0
- 0.688
- 0.798
- 0.8
- 0.002
- 0.781
- 0.787
- 0.0
- 0.787
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.804
- 0.0
- 0.0
- 0.0
- 0.0
- 0.808
- 0.0
- 0.827
- 0.823
- 0.0
- 0.0
- 0.0
- 0.942
- 0.812
- 0.0
- 0.0
- 0.81
- 0.0
- 0.0
- 0.81
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.831
- 0.0
- 0.002
- 0.823
- 0.933
- 0.0
- 0.0
- 0.0
- 0.0
- 0.817
- 0.833
- 0.004
- 0.0
- 0.0
- 0.0
train_loss:
- 2.052
- 1.277
- 1.7
- 1.51
- 0.902
- 0.833
- 0.797
- 1.168
- 1.103
- 1.038
- 1.373
- 0.687
- 0.975
- 0.963
- 0.645
- 0.944
- 0.637
- 0.876
- 0.881
- 0.877
- 1.098
- 1.09
- 0.812
- 0.579
- 0.344
- 0.843
- 0.796
- 0.781
- 0.525
- 0.759
- 0.308
- 0.537
- 0.985
- 0.296
- 0.481
- 0.951
- 0.478
- 0.471
- 0.278
- 0.681
- 0.452
- 0.457
- 0.667
- 0.488
- 0.429
- 0.479
- 0.678
- 0.251
- 0.634
- 0.424
- 0.455
- 0.642
- 0.449
- 0.435
- 0.612
- 0.608
- 0.604
- 0.418
- 0.436
- 0.592
- 0.604
- 0.592
- 0.423
- 0.427
- 0.405
- 0.592
- 0.407
- 0.554
- 0.606
- 0.557
- 0.419
- 0.403
- 0.237
- 0.579
- 0.557
- 0.409
- 0.565
- 0.39
- 0.545
- 0.536
- 0.379
- 0.719
- 0.405
- 0.554
- 0.548
- 0.704
- 0.393
- 0.374
- 0.538
- 0.223
- 0.332
- 0.355
- 0.389
- 0.378
- 0.539
- 0.377
- 0.365
- 0.531
- 0.362
- 0.351
unequal: 0
verbose: 1
