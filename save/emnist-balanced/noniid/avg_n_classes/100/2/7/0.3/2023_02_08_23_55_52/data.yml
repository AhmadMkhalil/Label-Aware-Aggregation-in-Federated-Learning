avg_train_accuracy: 0.781
avg_train_loss: 0.003
avg_type: avg_n_classes
dataset: emnist-balanced
epochs: 100
frac: 0.3
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 2
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.023617021276595745
- 0.062180851063829784
- 0.165
- 0.3118085106382979
- 0.38409574468085106
- 0.39941489361702126
- 0.41888297872340424
- 0.44335106382978723
- 0.4632978723404255
- 0.46180851063829786
- 0.4718085106382979
- 0.4804255319148936
- 0.4871276595744681
- 0.498936170212766
- 0.08542553191489362
- 0.4972872340425532
- 0.5047340425531915
- 0.506968085106383
- 0.5148936170212766
- 0.18345744680851064
- 0.5167021276595745
- 0.5257446808510639
- 0.5262234042553191
- 0.5323936170212766
- 0.20292553191489363
- 0.5351595744680852
- 0.5377659574468086
- 0.5372340425531915
- 0.5420744680851064
- 0.5426595744680851
- 0.5452659574468085
- 0.16595744680851063
- 0.5480851063829787
- 0.5487765957446809
- 0.549468085106383
- 0.27372340425531916
- 0.15643617021276596
- 0.5542553191489362
- 0.3896276595744681
- 0.1576595744680851
- 0.5577127659574468
- 0.556595744680851
- 0.556063829787234
- 0.5593085106382979
- 0.5581914893617022
- 0.5616489361702127
- 0.5617021276595745
- 0.5644148936170212
- 0.35297872340425535
- 0.5641489361702128
- 0.5675
- 0.4023404255319149
- 0.245
- 0.5685106382978723
- 0.5679787234042554
- 0.5690957446808511
- 0.5687765957446809
- 0.4554787234042553
- 0.5693617021276596
- 0.4681382978723404
- 0.2790957446808511
- 0.5726063829787233
- 0.5729787234042554
- 0.5745744680851064
- 0.5730851063829787
- 0.5727127659574468
- 0.5753191489361702
- 0.5707446808510638
- 0.5723936170212766
- 0.5715425531914894
- 0.5739893617021277
- 0.5257446808510639
- 0.5754255319148937
- 0.4537765957446809
- 0.5809574468085107
- 0.5796808510638298
- 0.5791489361702128
- 0.5181382978723404
- 0.5802127659574469
- 0.5802127659574469
- 0.5778191489361703
- 0.5797340425531915
- 0.5794680851063829
- 0.58
- 0.5793085106382979
- 0.5814893617021276
- 0.5751063829787234
- 0.583031914893617
- 0.5828723404255319
- 0.5808510638297872
- 0.5813297872340426
- 0.5807446808510638
- 0.5813297872340426
- 0.5795744680851064
- 0.5816489361702127
- 0.5799468085106383
- 0.32574468085106384
- 0.25840425531914896
- 0.5885106382978723
- 0.56
test_loss_list:
- 3.7990902360280354
- 3.8189607365926106
- 3.7201405270894368
- 3.477025655110677
- 3.3317113304138184
- 3.3094647534688315
- 3.2976955540974937
- 3.465491453806559
- 3.6750288645426434
- 3.4963120651245116
- 3.4040081850687662
- 3.4283928616841632
- 3.5711748695373533
- 3.659450289408366
- 4.853129844665528
- 3.0905445035298666
- 3.2977297846476237
- 3.188253157933553
- 3.2399561754862467
- 3.4464770921071373
- 3.0179549407958985
- 3.097151845296224
- 3.2305612246195476
- 3.414333511988322
- 3.638266464869181
- 2.9304352219899497
- 3.1564361604054767
- 3.1095620314280192
- 3.2466640218098957
- 3.1833541870117186
- 3.1468797079722086
- 3.9669321537017823
- 2.723831828435262
- 2.9482003815968834
- 3.073066415786743
- 2.897346992492676
- 3.8940133094787597
- 2.517276357014974
- 2.4117789300282797
- 4.118864180246989
- 2.33926073551178
- 2.4344082498550415
- 2.628812821706136
- 2.6047569942474365
- 2.6293528938293456
- 2.6948350429534913
- 2.711339832941691
- 2.6984967803955078
- 2.5405211003621417
- 2.591294221878052
- 2.81475949605306
- 2.1366170930862425
- 2.983007904688517
- 2.1873814344406126
- 2.348107573191325
- 2.459330129623413
- 2.5540242099761965
- 2.1504790004094443
- 2.1455728340148927
- 1.8637071577707927
- 2.9918178113301597
- 1.9862502241134643
- 2.256363967259725
- 2.2998059622446694
- 2.3674429003397623
- 2.444752154350281
- 2.5021129512786864
- 2.517522945404053
- 2.4508393224080405
- 2.4976175244649252
- 2.537655038833618
- 1.9350475136439005
- 2.1854851325352986
- 1.9211615737279255
- 2.0667769734064736
- 2.221354751586914
- 2.2843676741917927
- 1.8268961350123087
- 2.0367878818511964
- 2.130579897562663
- 2.1959276326497394
- 2.4069021352132163
- 2.4080107561747233
- 2.4487518660227456
- 2.39316987991333
- 2.384988859494527
- 1.56659015973409
- 1.980653551419576
- 2.152164176305135
- 2.285539518992106
- 2.226139270464579
- 2.3732001876831053
- 2.319789722760518
- 2.4021701526641848
- 2.411584974924723
- 2.346715415318807
- 2.7536617056528727
- 3.0054635492960613
- 2.0289000288645425
- 1.6217205540339152
train_accuracy:
- 0.0
- 0.062
- 0.237
- 0.452
- 0.588
- 0.0
- 0.0
- 0.633
- 0.652
- 0.0
- 0.0
- 0.0
- 0.721
- 0.735
- 0.76
- 0.744
- 0.74
- 0.0
- 0.0
- 0.283
- 0.0
- 0.773
- 0.0
- 0.798
- 0.227
- 0.0
- 0.781
- 0.0
- 0.0
- 0.0
- 0.0
- 0.542
- 0.781
- 0.802
- 0.0
- 0.85
- 0.983
- 0.0
- 0.91
- 0.933
- 0.0
- 0.781
- 0.0
- 0.0
- 0.833
- 0.777
- 0.004
- 0.79
- 0.869
- 0.844
- 0.844
- 0.65
- 0.175
- 0.0
- 0.833
- 0.0
- 0.804
- 0.892
- 0.0
- 0.723
- 0.74
- 0.01
- 0.804
- 0.798
- 0.0
- 0.808
- 0.0
- 0.844
- 0.0
- 0.812
- 0.0
- 0.946
- 0.0
- 0.598
- 0.8
- 0.008
- 0.0
- 0.981
- 0.0
- 0.065
- 0.0
- 0.823
- 0.0
- 0.0
- 0.84
- 0.0
- 0.99
- 0.0
- 0.838
- 0.0
- 0.004
- 0.865
- 0.856
- 0.86
- 0.0
- 0.0
- 0.894
- 0.825
- 0.863
- 0.781
train_loss:
- 1.903
- 1.742
- 1.68
- 1.451
- 1.973
- 1.08
- 1.097
- 1.51
- 1.926
- 0.892
- 1.038
- 0.887
- 0.871
- 1.237
- 0.57
- 0.931
- 0.892
- 0.876
- 0.779
- 0.49
- 0.643
- 0.757
- 0.742
- 1.033
- 0.431
- 1.064
- 0.935
- 0.654
- 0.989
- 0.663
- 0.718
- 0.387
- 0.598
- 0.594
- 0.63
- 0.295
- 0.399
- 0.579
- 0.365
- 0.272
- 0.562
- 0.685
- 0.552
- 0.625
- 0.652
- 0.578
- 0.616
- 0.619
- 0.288
- 0.879
- 0.768
- 0.299
- 0.238
- 0.647
- 0.525
- 0.526
- 0.496
- 0.248
- 0.649
- 0.224
- 0.266
- 0.56
- 0.83
- 0.497
- 0.517
- 0.494
- 0.784
- 0.494
- 0.501
- 0.515
- 0.486
- 0.239
- 0.53
- 0.35
- 0.797
- 0.799
- 0.473
- 0.299
- 0.444
- 0.558
- 0.491
- 0.709
- 0.448
- 0.699
- 0.475
- 0.55
- 0.273
- 0.489
- 0.452
- 0.437
- 0.587
- 0.692
- 0.533
- 0.445
- 0.438
- 0.475
- 0.264
- 0.275
- 0.758
- 0.29
unequal: 0
verbose: 1
