avg_train_accuracy: 0.915
avg_train_loss: 0.005
avg_type: avg
dataset: emnist-balanced
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 3
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.06202127659574468
- 0.15079787234042552
- 0.15824468085106383
- 0.15824468085106383
- 0.15824468085106383
- 0.15824468085106383
- 0.17707446808510638
- 0.1825
- 0.1825
- 0.18590425531914895
- 0.18909574468085105
- 0.18909574468085105
- 0.19180851063829787
- 0.19180851063829787
- 0.19340425531914893
- 0.19340425531914893
- 0.19340425531914893
- 0.19356382978723405
- 0.19930851063829788
- 0.19930851063829788
- 0.19930851063829788
- 0.2048936170212766
- 0.2048936170212766
- 0.2048936170212766
- 0.2048936170212766
- 0.2048936170212766
- 0.2048936170212766
- 0.2048936170212766
- 0.2048936170212766
- 0.20840425531914894
- 0.21936170212765957
- 0.21936170212765957
- 0.23627659574468085
- 0.23627659574468085
- 0.24117021276595746
- 0.24117021276595746
- 0.24117021276595746
- 0.24117021276595746
- 0.24117021276595746
- 0.24117021276595746
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.25367021276595747
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.2765957446808511
- 0.27930851063829787
- 0.27930851063829787
- 0.27930851063829787
- 0.27930851063829787
- 0.27930851063829787
- 0.27930851063829787
- 0.27930851063829787
- 0.285
- 0.285
- 0.285
- 0.285
- 0.285
- 0.285
- 0.285
- 0.285
- 0.285
- 0.285
test_loss_list:
- 623.8691501617432
- 1108.885579586029
- 1339.5778951644897
- 1339.5778951644897
- 1339.5778951644897
- 1339.5778951644897
- 1303.9509387016296
- 1354.8723821640015
- 1354.8723821640015
- 1463.7354764938354
- 1453.123161315918
- 1453.123161315918
- 1550.4007930755615
- 1550.4007930755615
- 1540.9413204193115
- 1540.9413204193115
- 1540.9413204193115
- 1316.5655708312988
- 910.8566799163818
- 910.8566799163818
- 910.8566799163818
- 883.763505935669
- 883.763505935669
- 883.763505935669
- 883.763505935669
- 883.763505935669
- 883.763505935669
- 883.763505935669
- 883.763505935669
- 894.760648727417
- 654.6819179058075
- 654.6819179058075
- 676.4469351768494
- 676.4469351768494
- 622.286135673523
- 622.286135673523
- 622.286135673523
- 622.286135673523
- 622.286135673523
- 622.286135673523
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 659.9874651432037
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 595.6463871002197
- 526.3557631969452
- 526.3557631969452
- 526.3557631969452
- 526.3557631969452
- 526.3557631969452
- 526.3557631969452
- 526.3557631969452
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
- 612.9410643577576
train_accuracy:
- 0.219
- 0.685
- 0.704
- 0.556
- 0.0
- 0.56
- 0.765
- 0.788
- 0.792
- 0.817
- 0.819
- 0.812
- 0.827
- 0.773
- 0.833
- 0.61
- 0.785
- 0.871
- 0.788
- 0.865
- 0.862
- 0.783
- 0.881
- 0.894
- 0.84
- 0.877
- 0.879
- 0.006
- 0.883
- 0.854
- 0.771
- 0.885
- 0.067
- 0.89
- 0.856
- 0.854
- 0.831
- 0.871
- 0.017
- 0.008
- 0.448
- 0.862
- 0.423
- 0.883
- 0.833
- 0.925
- 0.896
- 0.865
- 0.015
- 0.0
- 0.006
- 0.31
- 0.89
- 0.702
- 0.848
- 0.006
- 0.919
- 0.908
- 0.906
- 0.0
- 0.9
- 0.015
- 0.844
- 0.862
- 0.885
- 0.017
- 0.885
- 0.944
- 0.625
- 0.892
- 0.275
- 0.915
- 0.885
- 0.885
- 0.877
- 0.913
- 0.908
- 0.262
- 0.352
- 0.121
- 0.908
- 0.883
- 0.898
- 0.85
- 0.904
- 0.894
- 0.173
- 0.892
- 0.917
- 0.919
- 0.479
- 0.913
- 0.869
- 0.904
- 0.738
- 0.906
- 0.896
- 0.906
- 0.948
- 0.915
train_loss:
- 2.372
- 2.17
- 1.557
- 1.201
- 0.887
- 1.097
- 1.232
- 1.142
- 1.03
- 1.033
- 0.981
- 0.937
- 0.893
- 0.85
- 0.882
- 0.921
- 0.924
- 0.866
- 0.765
- 0.821
- 0.735
- 0.738
- 0.739
- 0.732
- 0.707
- 0.694
- 0.682
- 0.822
- 0.789
- 0.652
- 0.707
- 0.72
- 0.629
- 0.722
- 0.673
- 0.643
- 0.702
- 0.765
- 0.662
- 0.689
- 0.586
- 0.652
- 0.536
- 0.622
- 0.636
- 0.618
- 0.574
- 0.567
- 0.56
- 0.621
- 0.578
- 0.67
- 0.54
- 0.516
- 0.66
- 0.577
- 0.535
- 0.541
- 0.591
- 0.602
- 0.538
- 0.53
- 0.506
- 0.563
- 0.564
- 0.509
- 0.558
- 0.542
- 0.745
- 0.623
- 0.591
- 0.492
- 0.572
- 0.504
- 0.554
- 0.524
- 0.498
- 0.517
- 0.476
- 0.471
- 0.551
- 0.517
- 0.524
- 0.547
- 0.515
- 0.519
- 0.467
- 0.535
- 0.474
- 0.495
- 0.521
- 0.519
- 0.448
- 0.497
- 0.611
- 0.561
- 0.491
- 0.517
- 0.482
- 0.495
unequal: 0
verbose: 1
