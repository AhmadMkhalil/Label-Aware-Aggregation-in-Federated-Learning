avg_train_accuracy: 1.0
avg_train_loss: 0.001
avg_type: avg
dataset: emnist-balanced
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 0
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.02127659574468085
- 0.021329787234042552
- 0.025053191489361703
- 0.028563829787234044
- 0.0325
- 0.0325
- 0.0325
- 0.0325
- 0.03558510638297872
- 0.03558510638297872
- 0.0375
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.04356382978723404
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
- 0.0698936170212766
test_loss_list:
- 1281.6105308532715
- 1020.7228116989136
- 1122.6102538108826
- 997.8619198799133
- 878.6212491989136
- 878.6212491989136
- 878.6212491989136
- 878.6212491989136
- 904.07603931427
- 904.07603931427
- 868.5023322105408
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 841.3722853660583
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
- 934.0348787307739
train_accuracy:
- 0.0
- 0.0
- 0.969
- 0.504
- 0.952
- 0.004
- 0.0
- 0.008
- 0.969
- 0.0
- 0.983
- 0.542
- 1.0
- 0.99
- 0.002
- 0.771
- 0.0
- 0.969
- 1.0
- 1.0
- 0.008
- 0.858
- 0.717
- 0.987
- 0.123
- 0.74
- 0.898
- 0.0
- 0.475
- 0.292
- 0.048
- 0.938
- 0.017
- 0.167
- 0.871
- 0.0
- 0.0
- 0.0
- 0.046
- 1.0
- 0.308
- 0.792
- 0.0
- 0.94
- 0.04
- 0.0
- 0.0
- 0.017
- 0.0
- 0.0
- 0.873
- 0.05
- 0.006
- 0.181
- 0.0
- 0.0
- 0.36
- 0.306
- 0.831
- 0.033
- 0.0
- 0.35
- 0.821
- 0.862
- 0.702
- 0.715
- 0.64
- 0.0
- 0.979
- 0.0
- 0.525
- 0.621
- 0.0
- 0.044
- 0.048
- 0.933
- 1.0
- 0.006
- 0.998
- 0.996
- 0.415
- 0.75
- 0.715
- 0.004
- 0.317
- 1.0
- 0.998
- 0.85
- 0.117
- 0.123
- 0.927
- 0.01
- 0.262
- 0.002
- 0.0
- 0.012
- 0.138
- 0.433
- 0.913
- 1.0
train_loss:
- 0.265
- 0.174
- 0.145
- 0.154
- 0.17
- 0.147
- 0.12
- 0.183
- 0.157
- 0.105
- 0.147
- 0.144
- 0.143
- 0.103
- 0.169
- 0.123
- 0.136
- 0.118
- 0.125
- 0.142
- 0.156
- 0.117
- 0.112
- 0.122
- 0.123
- 0.129
- 0.092
- 0.17
- 0.098
- 0.147
- 0.102
- 0.111
- 0.132
- 0.124
- 0.074
- 0.131
- 0.124
- 0.104
- 0.125
- 0.159
- 0.095
- 0.099
- 0.099
- 0.108
- 0.09
- 0.105
- 0.074
- 0.123
- 0.08
- 0.143
- 0.087
- 0.103
- 0.114
- 0.124
- 0.081
- 0.117
- 0.106
- 0.09
- 0.089
- 0.084
- 0.117
- 0.114
- 0.104
- 0.115
- 0.095
- 0.091
- 0.099
- 0.117
- 0.097
- 0.075
- 0.123
- 0.118
- 0.063
- 0.114
- 0.092
- 0.098
- 0.075
- 0.094
- 0.139
- 0.097
- 0.083
- 0.078
- 0.05
- 0.107
- 0.069
- 0.118
- 0.11
- 0.089
- 0.08
- 0.077
- 0.12
- 0.092
- 0.091
- 0.095
- 0.062
- 0.072
- 0.102
- 0.079
- 0.111
- 0.094
unequal: 0
verbose: 1
