avg_train_accuracy: 0.171
avg_train_loss: 0.001
avg_type: avg
dataset: emnist-balanced
epochs: 100
frac: 0.1
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.5
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 47
number_of_classes_of_half_of_user: 0
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.02127659574468085
- 0.02127659574468085
- 0.02127659574468085
- 0.021329787234042552
- 0.021329787234042552
- 0.021382978723404257
- 0.021382978723404257
- 0.021382978723404257
- 0.021382978723404257
- 0.021382978723404257
- 0.02148936170212766
- 0.029308510638297872
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.04
- 0.040372340425531915
- 0.040372340425531915
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04542553191489362
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.04765957446808511
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
- 0.06340425531914894
test_loss_list:
- 1181.6983885765076
- 1181.6983885765076
- 1181.6983885765076
- 1307.1261568069458
- 1307.1261568069458
- 1145.4255619049072
- 1145.4255619049072
- 1145.4255619049072
- 1145.4255619049072
- 1145.4255619049072
- 1127.5797171592712
- 970.9317436218262
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 1105.3399405479431
- 869.4201879501343
- 869.4201879501343
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 983.0628142356873
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 877.0638446807861
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
- 985.9013814926147
train_accuracy:
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 1.0
- 0.0
- 0.0
- 0.0
- 0.404
- 0.862
- 0.006
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.983
- 0.979
- 1.0
- 1.0
- 0.5
- 0.0
- 0.0
- 0.998
- 0.942
- 1.0
- 0.023
- 0.0
- 0.823
- 1.0
- 0.879
- 0.71
- 0.985
- 0.0
- 0.502
- 0.15
- 0.462
- 0.86
- 0.048
- 0.965
- 0.535
- 1.0
- 1.0
- 0.594
- 0.0
- 0.429
- 0.065
- 1.0
- 0.0
- 0.0
- 0.0
- 0.0
- 0.029
- 0.6
- 0.962
- 0.773
- 0.992
- 0.246
- 1.0
- 0.042
- 0.975
- 0.823
- 0.948
- 0.171
- 0.025
- 0.05
- 0.592
- 0.0
- 1.0
- 0.981
- 0.575
- 0.0
- 0.235
- 0.0
- 0.317
- 0.0
- 0.998
- 0.992
- 0.267
- 0.0
- 0.677
- 0.75
- 0.0
- 0.0
- 1.0
- 0.544
- 0.0
- 0.277
- 0.35
- 0.569
- 0.842
- 1.0
- 0.0
- 0.06
- 0.171
train_loss:
- 0.373
- 0.137
- 0.189
- 0.125
- 0.194
- 0.112
- 0.144
- 0.11
- 0.17
- 0.144
- 0.131
- 0.164
- 0.108
- 0.161
- 0.125
- 0.147
- 0.144
- 0.146
- 0.141
- 0.124
- 0.118
- 0.156
- 0.145
- 0.1
- 0.105
- 0.163
- 0.122
- 0.128
- 0.122
- 0.116
- 0.095
- 0.15
- 0.101
- 0.123
- 0.142
- 0.088
- 0.122
- 0.13
- 0.149
- 0.114
- 0.162
- 0.103
- 0.109
- 0.139
- 0.129
- 0.113
- 0.112
- 0.1
- 0.113
- 0.09
- 0.128
- 0.118
- 0.129
- 0.108
- 0.104
- 0.11
- 0.107
- 0.081
- 0.1
- 0.073
- 0.121
- 0.117
- 0.123
- 0.103
- 0.11
- 0.124
- 0.09
- 0.084
- 0.132
- 0.1
- 0.11
- 0.102
- 0.113
- 0.099
- 0.121
- 0.101
- 0.093
- 0.105
- 0.095
- 0.112
- 0.1
- 0.12
- 0.076
- 0.115
- 0.101
- 0.115
- 0.066
- 0.101
- 0.107
- 0.117
- 0.094
- 0.107
- 0.102
- 0.099
- 0.096
- 0.092
- 0.105
- 0.074
- 0.098
- 0.091
unequal: 0
verbose: 1
