avg_train_accuracy: 0.854
avg_train_loss: 0.005
avg_type: avg
dataset: emnist-balanced
epochs: 100
frac: 0.7
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 6
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.028404255319148936
- 0.12079787234042554
- 0.15893617021276596
- 0.22452127659574467
- 0.2401595744680851
- 0.2526063829787234
- 0.24420212765957447
- 0.2802127659574468
- 0.24920212765957447
- 0.25101063829787235
- 0.2779255319148936
- 0.3237765957446809
- 0.3249468085106383
- 0.3503191489361702
- 0.2772340425531915
- 0.31882978723404254
- 0.28835106382978726
- 0.28691489361702127
- 0.3532446808510638
- 0.3632446808510638
- 0.3155851063829787
- 0.335
- 0.3798404255319149
- 0.36425531914893616
- 0.37824468085106383
- 0.32053191489361704
- 0.39547872340425533
- 0.3147340425531915
- 0.42351063829787233
- 0.3256914893617021
- 0.35297872340425535
- 0.42840425531914894
- 0.4111170212765957
- 0.445
- 0.3397872340425532
- 0.43648936170212765
- 0.4220212765957447
- 0.3536170212765957
- 0.44941489361702125
- 0.43186170212765956
- 0.3573404255319149
- 0.31063829787234043
- 0.44707446808510637
- 0.3520744680851064
- 0.4017553191489362
- 0.4430851063829787
- 0.4542553191489362
- 0.46691489361702126
- 0.371436170212766
- 0.4751595744680851
- 0.4824468085106383
- 0.33920212765957447
- 0.44351063829787235
- 0.37648936170212766
- 0.3278723404255319
- 0.4838297872340426
- 0.46287234042553194
- 0.47712765957446807
- 0.491436170212766
- 0.41819148936170214
- 0.5122340425531915
- 0.5490957446808511
- 0.5113297872340425
- 0.3424468085106383
- 0.4346276595744681
- 0.5162234042553191
- 0.40143617021276595
- 0.5268617021276596
- 0.40797872340425534
- 0.43372340425531913
- 0.43563829787234043
- 0.5142021276595745
- 0.428031914893617
- 0.4475
- 0.4211170212765957
- 0.46287234042553194
- 0.5252659574468085
- 0.4308510638297872
- 0.43867021276595747
- 0.4429787234042553
- 0.5419148936170213
- 0.5221808510638298
- 0.4512765957446809
- 0.5219680851063829
- 0.4425531914893617
- 0.5675
- 0.3596276595744681
- 0.5608510638297872
- 0.443563829787234
- 0.5227659574468085
- 0.3623936170212766
- 0.47393617021276596
- 0.4576063829787234
- 0.4778191489361702
- 0.5809042553191489
- 0.5381914893617021
- 0.5416489361702128
- 0.5578723404255319
- 0.5482978723404255
- 0.5635638297872341
test_loss_list:
- 3.9415506649017336
- 3.8460178979237876
- 3.6555127398173015
- 4.794528656005859
- 3.35509383837382
- 3.2600061734517416
- 3.452189906438192
- 3.052609723409017
- 3.5739182535807292
- 3.3334750652313234
- 3.312323802312215
- 2.949577372868856
- 3.055590387980143
- 2.9925814310709637
- 4.40951140721639
- 3.031494541168213
- 3.363522984186808
- 3.9411894861857095
- 2.824172172546387
- 2.949382184346517
- 3.3942005825042725
- 3.2600788434346515
- 2.7905539639790855
- 2.793749068578084
- 2.7378582604726156
- 3.208791888554891
- 2.6877194118499754
- 3.2594455528259276
- 2.581328919728597
- 3.2934909089406332
- 3.1672895050048826
- 2.5431006654103596
- 2.696543102264404
- 2.5293480809529623
- 3.261544036865234
- 2.511851329803467
- 2.6645433553059896
- 2.991992603937785
- 2.5212205918629964
- 2.5749673811594644
- 3.0548944727579754
- 3.8721399211883547
- 2.50689057191213
- 3.228239990870158
- 2.9060999902089435
- 2.5622516536712645
- 2.5861363697052
- 2.5755770301818846
- 2.9744007873535154
- 2.4875068108240765
- 2.5787168089548747
- 3.71382776260376
- 2.905332298278809
- 2.946323305765788
- 3.842531614303589
- 2.4254874261220296
- 2.544372943242391
- 2.506891574859619
- 2.5144289763768515
- 3.0279779720306395
- 2.392797632217407
- 2.3275472116470337
- 2.4836916494369508
- 3.722482941945394
- 2.758482205073039
- 2.4665473890304566
- 3.001565831502279
- 2.411038022041321
- 3.0358760261535647
- 2.8275615851084392
- 2.7633771228790285
- 2.5026182651519777
- 2.8781732749938964
- 2.9014097023010255
- 2.852895873387655
- 2.832337614695231
- 2.416560754776001
- 2.8916284974416095
- 2.823019701639811
- 2.854239823023478
- 2.4942178853352863
- 2.4670720704396567
- 2.844502124786377
- 2.456073447863261
- 2.9803701655069985
- 2.3135913610458374
- 3.832304360071818
- 2.421153817176819
- 3.046013174057007
- 2.4716569137573243
- 3.6721393489837646
- 2.809217363993327
- 2.8151842880249025
- 2.9138210264841717
- 2.308511872291565
- 2.481037220954895
- 2.4455279525121054
- 2.4629814990361534
- 2.439842519760132
- 2.3910340452194214
train_accuracy:
- 0.0
- 0.0
- 0.0
- 0.617
- 0.029
- 0.608
- 0.0
- 0.054
- 0.681
- 0.0
- 0.0
- 0.644
- 0.0
- 0.029
- 0.763
- 0.706
- 0.019
- 0.76
- 0.062
- 0.715
- 0.002
- 0.0
- 0.0
- 0.727
- 0.073
- 0.004
- 0.719
- 0.798
- 0.079
- 0.006
- 0.004
- 0.104
- 0.019
- 0.06
- 0.842
- 0.104
- 0.787
- 0.0
- 0.694
- 0.769
- 0.796
- 0.86
- 0.033
- 0.842
- 0.0
- 0.096
- 0.796
- 0.11
- 0.84
- 0.015
- 0.023
- 0.058
- 0.1
- 0.838
- 0.86
- 0.021
- 0.11
- 0.004
- 0.81
- 0.863
- 0.806
- 0.719
- 0.819
- 0.869
- 0.065
- 0.777
- 0.0
- 0.783
- 0.867
- 0.865
- 0.879
- 0.812
- 0.065
- 0.035
- 0.885
- 0.873
- 0.84
- 0.85
- 0.879
- 0.037
- 0.09
- 0.019
- 0.89
- 0.0
- 0.9
- 0.137
- 0.898
- 0.812
- 0.004
- 0.825
- 0.869
- 0.873
- 0.881
- 0.094
- 0.725
- 0.842
- 0.096
- 0.827
- 0.875
- 0.854
train_loss:
- 2.543
- 1.868
- 1.547
- 1.682
- 1.266
- 1.168
- 1.252
- 1.067
- 1.137
- 1.119
- 1.085
- 0.953
- 0.942
- 0.907
- 1.083
- 0.985
- 0.963
- 1.008
- 0.87
- 0.833
- 0.882
- 0.89
- 0.821
- 0.786
- 0.781
- 0.854
- 0.78
- 0.835
- 0.677
- 0.852
- 0.829
- 0.656
- 0.72
- 0.64
- 0.795
- 0.697
- 0.704
- 0.757
- 0.692
- 0.664
- 0.737
- 0.802
- 0.655
- 0.715
- 0.744
- 0.633
- 0.65
- 0.632
- 0.695
- 0.626
- 0.63
- 0.756
- 0.683
- 0.669
- 0.727
- 0.612
- 0.591
- 0.592
- 0.593
- 0.655
- 0.58
- 0.516
- 0.577
- 0.693
- 0.645
- 0.565
- 0.621
- 0.566
- 0.622
- 0.599
- 0.62
- 0.557
- 0.607
- 0.607
- 0.593
- 0.605
- 0.536
- 0.591
- 0.591
- 0.585
- 0.528
- 0.516
- 0.579
- 0.516
- 0.569
- 0.471
- 0.639
- 0.522
- 0.561
- 0.518
- 0.619
- 0.568
- 0.561
- 0.561
- 0.453
- 0.483
- 0.481
- 0.491
- 0.489
- 0.49
unequal: 0
verbose: 1
