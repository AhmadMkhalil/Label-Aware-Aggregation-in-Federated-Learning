avg_train_accuracy: 0.129
avg_train_loss: 0.004
avg_type: avg
dataset: emnist-balanced
epochs: 100
frac: 0.3
iid: 0
kernel_num: 9
kernel_sizes: 3,4,5
local_bs: 256
local_ep: 10
lr: 0.01
max_pool: 'True'
model: cnn
momentum: 0.2
norm: batch_norm
num_channels: 1
num_classes: 47
num_filters: 32
num_users: 10
number_of_classes_of_half_of_user: 6
optimizer: sgd
seed: 1
test_accuracy_list:
- 0.06851063829787234
- 0.11803191489361702
- 0.07585106382978724
- 0.1551595744680851
- 0.0973936170212766
- 0.1254255319148936
- 0.10787234042553191
- 0.13063829787234044
- 0.15313829787234043
- 0.09398936170212766
- 0.16106382978723405
- 0.17164893617021276
- 0.17106382978723406
- 0.09952127659574468
- 0.2123936170212766
- 0.2149468085106383
- 0.10835106382978724
- 0.14861702127659573
- 0.20154255319148937
- 0.1645744680851064
- 0.2150531914893617
- 0.10702127659574467
- 0.2147872340425532
- 0.1279787234042553
- 0.09957446808510638
- 0.20542553191489363
- 0.23175531914893616
- 0.22106382978723405
- 0.22170212765957448
- 0.26159574468085106
- 0.24824468085106383
- 0.2687765957446808
- 0.21574468085106382
- 0.2124468085106383
- 0.23202127659574467
- 0.24563829787234043
- 0.23601063829787233
- 0.2605851063829787
- 0.12606382978723404
- 0.22026595744680852
- 0.2401595744680851
- 0.25622340425531914
- 0.24601063829787234
- 0.27356382978723404
- 0.2647340425531915
- 0.28138297872340423
- 0.24712765957446808
- 0.259468085106383
- 0.24819148936170213
- 0.27101063829787236
- 0.2653191489361702
- 0.2720744680851064
- 0.15595744680851065
- 0.2626063829787234
- 0.2835106382978723
- 0.28
- 0.3203723404255319
- 0.3044148936170213
- 0.31042553191489364
- 0.3424468085106383
- 0.33
- 0.31196808510638296
- 0.33063829787234045
- 0.1747872340425532
- 0.19180851063829787
- 0.31579787234042556
- 0.2696808510638298
- 0.2847340425531915
- 0.3528723404255319
- 0.28797872340425534
- 0.1921808510638298
- 0.17898936170212765
- 0.2981382978723404
- 0.31553191489361704
- 0.30101063829787233
- 0.3332978723404255
- 0.18398936170212765
- 0.32574468085106384
- 0.2802127659574468
- 0.35319148936170214
- 0.3633510638297872
- 0.18111702127659574
- 0.326063829787234
- 0.3151595744680851
- 0.36606382978723406
- 0.3337765957446808
- 0.20101063829787233
- 0.3613297872340426
- 0.3249468085106383
- 0.39776595744680854
- 0.3435106382978723
- 0.37351063829787234
- 0.35404255319148936
- 0.34882978723404257
- 0.30601063829787234
- 0.3216489361702128
- 0.20553191489361702
- 0.3293617021276596
- 0.33313829787234045
- 0.3648936170212766
test_loss_list:
- 4.3430606396993
- 5.260796438852946
- 7.535976969401042
- 5.502996781667074
- 4.689366607666016
- 4.657115758260091
- 3.89391837755839
- 3.7673412863413493
- 4.040943441390991
- 12.798623072306315
- 3.757037022908529
- 3.6160080591837565
- 3.704923391342163
- 8.37133213043213
- 3.9845861275990804
- 4.031834398905437
- 6.324439487457275
- 5.179786224365234
- 4.011965084075928
- 3.9655754439036053
- 3.6395133177439374
- 6.564573815663656
- 3.5618945439656575
- 6.4002045631408695
- 11.965468254089355
- 4.267857968012492
- 3.656095403035482
- 3.9052320671081544
- 4.1644223658243815
- 3.461921351750692
- 3.961050484975179
- 3.357183272043864
- 4.421466499964396
- 4.952439804077148
- 3.8105687872568765
- 4.19419708887736
- 4.333787345886231
- 3.8650964895884194
- 6.361400566101074
- 4.810555076599121
- 4.800744876861573
- 3.331192315419515
- 4.2234221458435055
- 3.4885039297739664
- 3.6559039147694907
- 3.3699123001098634
- 3.9787459659576414
- 4.528638273874918
- 3.9731888802846274
- 3.742488479614258
- 4.256968673070272
- 3.600524727503459
- 6.182488842010498
- 3.9311812623341877
- 3.7862434069315594
- 3.95198379834493
- 3.445264987945557
- 3.6887976868947345
- 3.840125567118327
- 3.29288737932841
- 3.0976911067962645
- 3.588923556009928
- 3.5309056091308593
- 5.73109265645345
- 5.479700063069662
- 3.5067505709330242
- 3.9739923254648843
- 3.744221337636312
- 3.154965318044027
- 4.230398581822713
- 4.7691114807128905
- 6.4083630561828615
- 3.7412680912017824
- 3.499521582921346
- 4.154713935852051
- 3.74287589708964
- 5.6309728368123375
- 3.4592256291707355
- 4.1217555236816406
- 3.042138115564982
- 3.079962504704793
- 5.6663753700256345
- 3.3990545336405438
- 3.995530831019084
- 3.3509436639149985
- 3.6236250432332358
- 4.664997908274333
- 3.5216560204823812
- 3.661130247116089
- 3.001416107813517
- 3.441964693069458
- 3.1013704840342204
- 3.5956294473012287
- 3.5769572766621907
- 4.0351382319132485
- 3.7721782779693602
- 5.10638048171997
- 3.786340030034383
- 3.9152701536814374
- 3.5718857765197756
train_accuracy:
- 0.046
- 0.11
- 0.704
- 0.044
- 0.019
- 0.71
- 0.142
- 0.002
- 0.096
- 0.885
- 0.137
- 0.088
- 0.075
- 0.0
- 0.612
- 0.085
- 0.881
- 0.006
- 0.133
- 0.065
- 0.025
- 0.89
- 0.6
- 0.0
- 0.921
- 0.154
- 0.035
- 0.096
- 0.096
- 0.075
- 0.085
- 0.144
- 0.56
- 0.588
- 0.123
- 0.1
- 0.652
- 0.783
- 0.921
- 0.06
- 0.092
- 0.16
- 0.692
- 0.081
- 0.137
- 0.113
- 0.123
- 0.867
- 0.075
- 0.71
- 0.117
- 0.831
- 0.002
- 0.065
- 0.129
- 0.121
- 0.11
- 0.117
- 0.129
- 0.113
- 0.16
- 0.117
- 0.119
- 0.942
- 0.05
- 0.852
- 0.085
- 0.094
- 0.154
- 0.133
- 0.948
- 0.946
- 0.835
- 0.158
- 0.125
- 0.142
- 0.948
- 0.713
- 0.675
- 0.081
- 0.127
- 0.952
- 0.125
- 0.133
- 0.121
- 0.121
- 0.933
- 0.137
- 0.133
- 0.14
- 0.075
- 0.115
- 0.104
- 0.133
- 0.146
- 0.144
- 0.119
- 0.133
- 0.108
- 0.129
train_loss:
- 1.989
- 1.179
- 1.212
- 0.929
- 1.008
- 0.894
- 0.903
- 0.935
- 0.909
- 0.77
- 1.001
- 0.902
- 0.719
- 0.619
- 0.689
- 0.665
- 0.63
- 0.546
- 0.797
- 0.831
- 0.716
- 0.641
- 0.614
- 0.502
- 0.437
- 0.588
- 0.685
- 0.633
- 0.561
- 0.628
- 0.519
- 0.571
- 0.633
- 0.552
- 0.574
- 0.49
- 0.532
- 0.462
- 0.499
- 0.595
- 0.457
- 0.571
- 0.493
- 0.521
- 0.568
- 0.497
- 0.5
- 0.405
- 0.48
- 0.407
- 0.379
- 0.493
- 0.371
- 0.507
- 0.475
- 0.427
- 0.474
- 0.434
- 0.396
- 0.478
- 0.406
- 0.38
- 0.391
- 0.349
- 0.37
- 0.421
- 0.446
- 0.438
- 0.395
- 0.317
- 0.381
- 0.319
- 0.412
- 0.443
- 0.345
- 0.419
- 0.312
- 0.454
- 0.369
- 0.423
- 0.36
- 0.323
- 0.429
- 0.361
- 0.396
- 0.379
- 0.374
- 0.388
- 0.384
- 0.416
- 0.336
- 0.429
- 0.332
- 0.349
- 0.322
- 0.323
- 0.361
- 0.338
- 0.315
- 0.353
unequal: 0
verbose: 1
